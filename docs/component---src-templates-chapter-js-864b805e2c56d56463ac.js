(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{684:function(e,n,t){"use strict";t.r(n);var a=t(0),o=t.n(a),r=t(686),s=t(819),i=t.n(s),l=t(820),c=t.n(l),m=(t(156),t(157),t(689)),d=t.n(m),h=(t(838),t(840)),u=t.n(h),p=t(669),g=t.n(p),f=function(e){var n,t=e.Component,a=void 0===t?"button":t,r=e.children,s=e.onClick,i=e.variant,l=e.small,c=e.className,m=d()(g.a.root,c,((n={})[g.a.primary]="primary"===i,n[g.a.secondary]="secondary"===i,n[g.a.small]=!!l,n));return o.a.createElement(a,{className:m,onClick:s},r)},w=function(e){var n,t=e.completed,a=e.toggleComplete,r=e.small,s=void 0===r||r,i=d()(((n={})[g.a.completeInactive]=!t,n[g.a.completeActive]=t,n));return o.a.createElement(f,{small:s,onClick:a,className:i},t?o.a.createElement(o.a.Fragment,null,o.a.createElement(u.a,{width:14,height:14,className:g.a.completeIcon})," ",o.a.createElement("span",{className:g.a.completeLabel},"Completed")," ",o.a.createElement("span",{className:g.a.completeLabelHover},"Remove from completed")):"Mark as completed")},_=o.a.createContext(),b=t(841),y=t.n(b),v=t(670),x=t.n(v),k=function(e){var n,t,r,s=e.id,i=e.title,l=e.type,c=e.children,m=Object(a.useRef)(),h=parseInt(s),u=Object(a.useContext)(_),p=u.activeExc,g=u.setActiveExc,b=u.completed,v=u.setCompleted,k=p===h,M=b.includes(h);Object(a.useEffect)(function(){k&&m.current&&m.current.scrollIntoView()},[k]);var E=Object(a.useCallback)(function(){return g(k?null:h)},[k,h]),R=Object(a.useCallback)(function(){return g(h+1)}),N=Object(a.useCallback)(function(){var e=M?b.filter(function(e){return e!==h}):[].concat(b,[h]);v(e)},[M,b,h]),A=d()(x.a.root,((n={})[x.a.expanded]=k,n[x.a.wide]=k&&"slides"===l,n[x.a.completed]=!k&&M,n)),S=d()(x.a.title,((t={})[x.a.titleExpanded]=k,t));return o.a.createElement("section",{ref:m,id:s,className:A},o.a.createElement("h2",{className:S,onClick:E},o.a.createElement("span",null,o.a.createElement("span",{className:d()(x.a.id,(r={},r[x.a.idCompleted]=M,r))},h),i),"slides"===l&&o.a.createElement(y.a,{className:x.a.icon})),k&&o.a.createElement("div",null,c,o.a.createElement("footer",{className:x.a.footer},o.a.createElement(w,{completed:M,toggleComplete:N}),o.a.createElement(f,{onClick:R,variant:"secondary",small:!0},"Next"))))},M=t(20),E=t.n(M),R=(t(98),t(842),t(97),t(843)),N=t(671),A=t.n(N),S=function(e){var n=e.expanded,t=void 0!==n&&n,r=e.actions,s=void 0===r?[]:r,i=e.children,l=Object(a.useState)(t),c=l[0],m=l[1],d=Object(a.useCallback)(function(){return m(!c)},[c]);return o.a.createElement("aside",{className:A.a.root},c&&i&&o.a.createElement("div",{className:A.a.content},i),o.a.createElement("menu",{className:A.a.actions},i&&o.a.createElement("button",{className:A.a.label,onClick:d},c?"Hide hints":"Show hints"),s.map(function(e,n){var t=e.text,a=e.onClick;return o.a.createElement("button",{className:A.a.label,key:n,onClick:a},t)})))},T=t(672),L=t.n(T);var F=function(e){function n(){for(var n,t=arguments.length,a=new Array(t),o=0;o<t;o++)a[o]=arguments[o];return(n=e.call.apply(e,[this].concat(a))||this).state={Juniper:null,showSolution:!1,key:0},n}E()(n,e);var a=n.prototype;return a.handleShowSolution=function(){this.setState({showSolution:!0})},a.handleReset=function(){this.setState({showSolution:!1,key:this.state.key+1})},a.updateJuniper=function(){var e=this;this.state.Juniper||Promise.resolve().then(t.bind(null,331)).then(function(n){e.setState({Juniper:n.default})})},a.componentDidMount=function(){this.updateJuniper()},a.componentDidUpdate=function(){this.updateJuniper()},a.render=function(){var e=this,n=this.state,t=n.Juniper,a=n.showSolution,s=this.props,i=s.id,l=s.source,c=s.solution,m=s.test,d=s.children,h=l||"exc_"+i,u=c||"solution_"+i,p=m||"test_"+i,g={cell:L.a.cell,input:L.a.input,button:L.a.button,output:L.a.output},w=[{text:"Show solution",onClick:function(){return e.handleShowSolution()}},{text:"Reset",onClick:function(){return e.handleReset()}}];return o.a.createElement(r.b,{query:"1719272222",render:function(n){var r,s=n.site.siteMetadata.testTemplate,i=n.site.siteMetadata.juniper,l=i.repo,c=i.branch,m=i.kernelType,_=i.debug,b=i.lang,y=(r=n.allCode,Object.assign.apply(Object,[{}].concat(r.edges.map(function(e){var n,t=e.node;return(n={})[t.name]=t.code,n})))),v=y[h],x=y[u],k=y[p];return o.a.createElement("div",{className:L.a.root,key:e.state.key},t&&o.a.createElement(t,{msgButton:null,classNames:g,repo:l,branch:c,lang:b,kernelType:m,debug:_,actions:function(e){var n=e.runCode;return o.a.createElement(o.a.Fragment,null,o.a.createElement(f,{onClick:function(){return n()}},"Run Code"),k&&o.a.createElement(f,{variant:"primary",onClick:function(){return n(function(e){return function(e,n,t,a){var o=a.replace(/"/g,'\\"'),r=a.replace(/"/g,'\\"');return e.replace(/\${solutionEscaped}/g,o).replace(/\${solutionFileEscaped}/g,r).replace(/\${solution}/g,a).replace(/\${test}/g,n)}(s,k,0,e)})}},"Submit"))}},a?x:v),o.a.createElement(S,{actions:w},d))},data:R})},n}(o.a.Component),G=t(692),I=(t(155),t(844)),C=t(714),z=t.n(C),P=(t(674),t(675)),H=t.n(P);function W(e,n){var t;return((t=e.allMarkdownRemark,Object.assign.apply(Object,[{}].concat(t.edges.map(function(e){var n,t=e.node;return(n={})[t.fields.slug.replace("/","")]=t.rawMarkdownBody,n}))))[n]||"").split("\n---\n").map(function(e){return e.trim()})}var U=function(e){function n(){return e.apply(this,arguments)||this}E()(n,e);var a=n.prototype;return a.componentDidMount=function(){Promise.resolve().then(t.t.bind(null,845,7)).then(function(e){var n=e.default;window.Reveal=n,window.marked=z.a,Promise.resolve().then(t.t.bind(null,846,7)).then(function(e){e.RevealMarkdown.init(),n.initialize({center:!1,progress:!1,showNotes:!0,controls:!0,width:"100%",height:600,minScale:.75,maxScale:1})})})},a.componentWillUnmount=function(){delete window.Reveal,delete window.marked,delete t.c[845],delete t.c[846]},a.render=function(){var e=this.props.source,n=d()("reveal","show-notes",H.a.reveal),t=d()("slides",H.a.slides);return o.a.createElement("div",{className:H.a.root},o.a.createElement("div",{className:n},o.a.createElement(r.b,{query:"1922309225",render:function(n){var a=W(n,e);return o.a.createElement("div",{className:t},a.map(function(e,n){return o.a.createElement("section",{key:n,"data-markdown":"","data-separator-notes":"^Notes:"},o.a.createElement("textarea",{"data-template":!0,defaultValue:e}))}))},data:I})))},n}(o.a.Component),D=t(676),O=t.n(D),j=function(e){var n=e.id,t=void 0===n?"0":n,r=e.children,s=void 0===r?[]:r,i=Object(a.useState)(null),l=i[0],c=i[1],m=Object(a.useState)(null),h=m[0],u=m[1],p=Object(a.useCallback)(function(){return u(l)},[l]),g=s.filter(function(e){return"\n"!==e});return o.a.createElement(o.a.Fragment,null,g.map(function(e,n){var a=e.key,r=e.props;return o.a.createElement("p",{key:a,className:O.a.option},o.a.createElement("input",{className:O.a.input,name:"choice-"+t,id:"choice-"+t+"-"+n,value:n,type:"radio",checked:l===n,onChange:function(){return c(n)}}),o.a.createElement("label",{className:O.a.label,htmlFor:"choice-"+t+"-"+n,dangerouslySetInnerHTML:{__html:"<span>"+r.text+"</span>"}}))}),o.a.createElement(f,{variant:"primary",onClick:p},"Submit"),g.map(function(e,n){var t,a,r=e.key,s=e.props,i=!!s.correct;return h===n?o.a.createElement("div",{key:r,className:d()(O.a.answer,(t={},t[O.a.correct]=i,t))},o.a.createElement("strong",{className:d()(O.a.answerLabel,(a={},a[O.a.answerLabelCorrect]=i,a))},i?"That's correct! ":"Incorrect. "),s.children):null}))},B=t(715),q=new c.a({createElement:o.a.createElement,components:{exercise:k,slides:U,codeblock:F,choice:j,opt:function(e){return e.children},a:G.a,hr:B.b,h3:B.a,ol:B.e,ul:B.f,li:B.d,code:B.c}}).Compiler,$=t(718),V=t(680),Y=t.n(V);t.d(n,"pageQuery",function(){return X});n.default=function(e){var n=e.data,t=n.markdownRemark,s=n.site.siteMetadata.courseId,l=t.frontmatter,c=t.htmlAst,m=l.title,d=l.description,h=l.prev,u=l.next,p=l.id,g=Object(a.useState)(null),w=g[0],b=g[1],y=i()(s+"-completed-"+p,[]),v=y[0],x=y[1],k=q(c),M=[{slug:h,text:"« Previous Chapter"},{slug:u,text:"Next Chapter »"}];return o.a.createElement(_.Provider,{value:{activeExc:w,setActiveExc:b,completed:v,setCompleted:x}},o.a.createElement($.a,{title:m,description:d},k,o.a.createElement("section",{className:Y.a.pagination},M.map(function(e){var n=e.slug,t=e.text;return o.a.createElement("div",{key:n},n&&o.a.createElement(f,{variant:"secondary",small:!0,onClick:function(){return Object(r.c)(n)}},t))}))))};var X="3199094430"},840:function(e,n,t){var a=t(0);function o(e){return a.createElement("svg",e,a.createElement("path",{d:"M9 16.172l10.594-10.594 1.406 1.406-12 12-5.578-5.578 1.406-1.406z"}))}o.defaultProps={width:"24",height:"24",viewBox:"0 0 24 24",fill:"currentColor"},e.exports=o,o.default=o},841:function(e,n,t){var a=t(0);function o(e){return a.createElement("svg",e,[a.createElement("path",{d:"M23.8 2.5A2.5 2.5 0 0 0 21.3 0H2.5A2.5 2.5 0 0 0 0 2.5v9.9a2.5 2.5 0 0 0 2.3 2.5v-1A1.5 1.5 0 0 1 1 12.4V2.5A1.5 1.5 0 0 1 2.5 1h18.8a1.5 1.5 0 0 1 1.5 1.5v3h1zM2.3 13.9v1z M23.5 20.3H4.8a2.5 2.5 0 0 1-2.5-2.5V8a2.5 2.5 0 0 1 2.5-2.5h18.7A2.5 2.5 0 0 1 26 8v9.8a2.5 2.5 0 0 1-2.5 2.5zM4.8 6.5A1.4 1.4 0 0 0 3.3 8v9.8a1.4 1.4 0 0 0 1.5 1.5h18.7a1.4 1.4 0 0 0 1.5-1.5V8a1.4 1.4 0 0 0-1.5-1.5z M15.8 9.5h7.1v1h-7.1z M15.8 12.4h7.1v1h-7.1z M15.9 15.3H23v1h-7.1z M4.9 10h1.5v6.5H4.9zM7.4 11.7h1.5v4.74H7.4zM10 11.7h1.5v4.74H10z",key:0}),a.createElement("circle",{cx:"14.1",cy:"10",r:"0.7",key:1}),a.createElement("circle",{cx:"14.1",cy:"12.9",r:"0.7",key:2}),a.createElement("circle",{cx:"14.2",cy:"15.8",r:"0.7",key:3})])}o.defaultProps={width:"24",height:"32",viewBox:"0 0 26 17",fill:"currentColor"},e.exports=o,o.default=o},843:function(e){e.exports={data:{site:{siteMetadata:{testTemplate:'options(error=traceback)\nlibrary(testwhat)\n.solution <- "${solutionEscaped}"\n\n.solutionfile <- "${solutionFileEscaped}"\n\n\nsetup_state(sol_code = .solutionfile, stu_code = .solution)\n\ntryCatch({\n    ${test}\n    cat(paste("\\033[32m", tw$get()$success_msg, "\\033[0m", sep = ""))\n}, error = function(e) {\n    cat(paste("\\033[31m", e[1], "\\033[0m", sep = ""))\n})',juniper:{repo:"noamross/gams-in-r-course",branch:"master",kernelType:"ir",lang:"r",debug:!1}}},allCode:{edges:[{node:{name:"exc_01_04",code:"library(mgcv)\n\nmcycle <- MASS::mcycle\ngam_mod <- mgcv::gam(accel ~ s(times), data=mcycle)\n\n# Extract the model coefficients\ncoef(____)"}},{node:{name:"exc_01_03",code:"mcycle <- MASS::mcycle\n\n# Load mgcv\n___\n\n# Fit the model\ngam_mod <- gam(___ ~ s(___), data = mcycle)\n\n# Plot the results\nplot(gam_mod, residuals = TRUE, pch = 1)"}},{node:{name:"exc_01_02",code:"mcycle <- MASS::mcycle\n\n# Examine the mcycle data frame\nhead(___)\nplot(___)\n\n# Fit a linear model\nlm_mod <- lm(___, data = mcycle)\n\n# Visualize the model\ntermplot(lm_mod, partial.resid = TRUE, se = TRUE)"}},{node:{name:"exc_01_06",code:"mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Fit a GAM with 3 basis functions\ngam_mod_k3 <- gam(accel ~ s(times, k = ___), data = mcycle)\n\n# Fit with 20 basis functions\ngam_mod_k20 <- gam(___)\n\n# Visualize the GAMs\npar(mfrow = c(1, 2))\nplot(gam_mod_k3, residuals = TRUE, pch = 1)\nplot(gam_mod_k20, residuals = TRUE, pch = 1)"}},{node:{name:"exc_01_10",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Examine the data\n___\n___\n\n# Fit the model\nmod_city <- gam(city.mpg ~ ___, \n                data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city, pages = 1)'}},{node:{name:"exc_01_11",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Fit the model\nmod_city2 <- gam(city.mpg ~ s(weight) + s(length) + s(price) ___, \n                 data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city2, all.terms = TRUE, pages = 1)'}},{node:{name:"exc_01_07",code:'mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Extract the smoothing parameter\ngam_mod <- gam(accel ~ s(times), data = mcycle, method = "REML")\n___\n\n# Fix the smoothing parameter at 0.1\ngam_mod_s1 <- gam(accel ~ s(times), data = mcycle, sp = ___)\n\n# Fix the smoothing parameter at 0.0001\ngam_mod_s2 <- gam(___)\n\n# Plot both models\npar(mfrow = c(2, 1))\nplot(gam_mod_s1, residuals = TRUE, pch = 1)\nplot(gam_mod_s2, residuals = TRUE, pch = 1)'}},{node:{name:"exc_01_08",code:"mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Fit the GAM\ngam_mod_sk <- ___\n\n# Visualize the model\nplot(gam_mod_sk, residuals = TRUE, pch = 1)"}},{node:{name:"exc_02_05",code:'mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(accel ~ s(times), data = mcycle, method = "REML")\n\n# Make the plot with residuals\nplot(mod, ___)\n\n# Change shape of residuals\n___'}},{node:{name:"exc_01_12",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Fit the model\nmod_city3 <- gam(city.mpg ~ ___, \n                 data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city3, pages = 1)'}},{node:{name:"exc_02_07",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, \n           data = mpg, method = "REML")\n\n# Plot the weight effect with colored shading\nplot(mod, select = 1, ___)\n\n# Make another plot adding the intercept value and uncertainty\nplot(mod, select = 1, ___)\n\n'}},{node:{name:"exc_02_06",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, \n           data = mpg, method = "REML")\n\n# Plot the price effect\n___\n\n# Plot all effects\n___'}},{node:{name:"exc_03_03",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\nmod2da <- gam(cadmium ~ s(x, y) ___, \n              data = meuse, method = "REML")\n\n# Inspect the model\nsummary(mod2da)\n'}},{node:{name:"exc_03_06",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make the perspective plot with error surfaces\nvis.gam(mod2d, view = c("x", "y"), \n        ___)\n\n'}},{node:{name:"exc_03_07",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 5% extrapolation\nvis.gam(mod2d, view = c("x", "y"), \n        plot.type = "contour", ___)\n\n# Overlay data\npoints(meuse)\n\n'}},{node:{name:"exc_03_09",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit a model with separate smooths for each land-use level\nmod_sep <- ___\n\n# Examine the summary\n___\n'}},{node:{name:"exc_03_10",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod_sep <- gam(copper ~ s(dist, by=landuse), data=meuse, method = "REML")\nmod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data=meuse, method = "REML")\n\n# Plot both the models with plot()\n___\n___\n\n'}},{node:{name:"exc_03_12",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\ntensor_mod <- ___\n\n# Summarize and plot\n___\n___\n\n'}},{node:{name:"exc_03_13",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\ntensor_mod2 <- ___\n\n\n# Summarize and plot\n___\n___(___, pages = 1)\n\n'}},{node:{name:"solution_01_02",code:"mcycle <- MASS::mcycle\n\n# Examine the mcycle data frame\nhead(mcycle)\nplot(mcycle)\n\n# Fit a linear model\nlm_mod <- lm(accel ~ times, data = mcycle)\n\n# Visualize the model\ntermplot(lm_mod, partial.resid = TRUE, se = TRUE)"}},{node:{name:"solution_01_03",code:"mcycle <- MASS::mcycle\n\n# Load mgcv\nlibrary(mgcv)\n\n# Fit the model\ngam_mod <- gam(accel ~ s(times), data = mcycle)\n\n# Plot the results\nplot(gam_mod, residuals = TRUE, pch = 1)"}},{node:{name:"solution_01_04",code:"library(mgcv)\n\nmcycle <- MASS::mcycle\ngam_mod <- mgcv::gam(accel ~ s(times), data=mcycle)\n\n# Extract the model coefficients\ncoef(gam_mod)"}},{node:{name:"solution_01_06",code:"mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Fit a GAM with 3 basis functions\ngam_mod_k3 <- gam(accel ~ s(times, k = 3), data = mcycle)\n\n# Fit with 20 basis functions\ngam_mod_k20 <- gam(accel ~ s(times, k = 20), data = mcycle)\n\n# Visualize the GAMs\npar(mfrow = c(1, 2))\nplot(gam_mod_k3, residuals = TRUE, pch = 1)\nplot(gam_mod_k20, residuals = TRUE, pch = 1)"}},{node:{name:"solution_01_07",code:'mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Extract the smoothing parameter\ngam_mod <- gam(accel ~ s(times), data = mcycle, method = "REML")\ngam_mod$sp\n\n# Fix the smoothing parameter at 0.1\ngam_mod_s1 <- gam(accel ~ s(times), data = mcycle, sp = 0.1)\n\n# Fix the smoothing parameter at 0.0001\ngam_mod_s2 <- gam(accel ~ s(times), data = mcycle, sp = 0.0001)\n\n# Plot both models\npar(mfrow = c(2, 1))\nplot(gam_mod_s1, residuals = TRUE, pch = 1)\nplot(gam_mod_s2, residuals = TRUE, pch = 1)'}},{node:{name:"solution_01_10",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Examine the data\nhead(mpg)\nstr(mpg)\n\n# Fit the model\nmod_city <- gam(city.mpg ~ s(weight) + s(length) + s(price), \n                data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city, pages = 1)'}},{node:{name:"solution_01_08",code:"mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n\n# Fit the GAM\ngam_mod_sk <- gam(accel ~ s(times, k = 50), data = mcycle, sp = 0.0001)\n\n# Visualize the model\nplot(gam_mod_sk, residuals = TRUE, pch = 1)"}},{node:{name:"solution_01_11",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Fit the model\nmod_city2 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + fuel + drive + style,\n                 data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city2, all.terms = TRUE, pages = 1)'}},{node:{name:"solution_01_12",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n\n# Fit the model\nmod_city3 <- gam(city.mpg ~ s(weight, by = drive) + s(length, by = drive) + s(price, by = drive) + drive,\n                 data = mpg, method = "REML")\n\n# Plot the model\nplot(mod_city3, pages = 1)\n'}},{node:{name:"solution_02_05",code:'mcycle <- MASS::mcycle\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(accel ~ s(times), data = mcycle, method = "REML")\n\n# Make the plot with residuals\nplot(mod, residuals = TRUE)\n\n# Change shape of residuals\nplot(mod, residuals = TRUE, pch = 1, cex = 1)'}},{node:{name:"solution_02_06",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, \n           data = mpg, method = "REML")\n\n# Plot the price effect\nplot(mod, select = 3)\n\n# Plot all effects\nplot(mod, pages = 1, all.terms = TRUE)'}},{node:{name:"solution_02_07",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, \n           data = mpg, method = "REML")\n\n# Plot the weight effect\nplot(mod, select = 1, shade = TRUE, shade.col = "hotpink")\n\n# Make another plot adding the intercept value and uncertainty\nplot(mod, select = 1, shade = TRUE, shade.col = "hotpink", \n     shift = coef(mod)[1], seWithMean = TRUE)'}},{node:{name:"solution_03_03",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev), \n              data = meuse, method = "REML")\n\n# Inspect the model\nsummary(mod2da)\n'}},{node:{name:"solution_03_06",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make the perspective plot with error surfaces\nvis.gam(mod2d, view = c("x", "y"), \n        plot.type = "persp", se = 2)\n'}},{node:{name:"solution_03_07",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 5% extrapolation\nvis.gam(mod2d, view = c("x", "y"), \n        plot.type = "contour", too.far = 0.05)\n\n# Overlay data\npoints(meuse)'}},{node:{name:"solution_03_10",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod_sep <- gam(copper ~ s(dist, by=landuse), data=meuse, method = "REML")\nmod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data=meuse, method = "REML")\n\n# Plot both the models with plot()\nplot(mod_sep, pages = 1)\nplot(mod_fs, pages = 1)\n\n'}},{node:{name:"solution_03_12",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\ntensor_mod <- gam(cadmium ~ te(x, y, elev), \n                  data = meuse, method = "REML")\n\n# Summarize and plot\nsummary(tensor_mod)\nplot(tensor_mod)\n'}},{node:{name:"solution_03_13",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\ntensor_mod2 <- gam(cadmium ~ s(x, y) + s(elev) + ti(x, y, elev), \n                   data = meuse, method = "REML")\n\n# Summarize and plot\nsummary(tensor_mod2)\nplot(tensor_mod2, pages = 1)\n'}},{node:{name:"exc_02_02",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod_city4 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + s(rpm) + s(width),\n                 data = mpg, method = "REML")\n\n# View the summary\n___\n'}},{node:{name:"solution_02_02",code:'library(gamair)\ndata("mpg", package="gamair")\n\nlibrary(mgcv)\n# Fit the model\nmod_city4 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + s(rpm) + s(width),\n                 data = mpg, method = "REML")\n\n# View the summary\nsummary(mod_city4)\n'}},{node:{name:"exc_02_09",code:'library(mgcv)\nset.seed(0)\ndat <- gamSim(1,n=200)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(y ~ s(x0, k = 5) + s(x1, k = 5) + s(x2, k = 5) + s(x3, k = 5),\n           data = dat, method = "REML")\n\n# Run the check function\n___\n'}},{node:{name:"solution_02_09",code:'library(mgcv)\nset.seed(0)\ndat <- gamSim(1,n=200)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(y ~ s(x0, k = 5) + s(x1, k = 5) + s(x2, k = 5) + s(x3, k = 5),\n           data = dat, method = "REML")\n\n# Run the check function\ngam.check(mod)\n'}},{node:{name:"exc_02_10",code:'set.seed(0)\ndat <- mgcv::gamSim(1,n=600, scale=0.6, verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 3) + s(x3, k = 3),\n           data = dat, method = "REML")\n\n# Check the diagnostics\n___\n\n# Refit to fix issues\nmod2 <- gam(y ~ s(x0, k = ___) + s(x1, k = ___) + s(x2, k = ___) + s(x3, k = ___),\n           data = dat, method = "REML")\n\n# Check the new model\n___\n'}},{node:{name:"solution_02_10",code:'set.seed(0)\ndat <- mgcv::gamSim(1,n=600, scale=0.6, verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 3) + s(x3, k = 3),\n           data = dat, method = "REML")\n\n# Check the diagnostics\ngam.check(mod)\n\n# Refit to fix issues\nmod2 <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 10) + s(x3, k = 3),\n           data = dat, method = "REML")\n\n# Check the new model\ngam.check(mod2)\n'}},{node:{name:"solution_02_12",code:'library(gamair)\nset.seed(0)\ndata("mpg", package="gamair", verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight),\n           data = mpg, method = "REML")\n\n# Check overall concurvity\nconcurvity(mod, full = TRUE)\n'}},{node:{name:"exc_02_12",code:'library(gamair)\nset.seed(0)\ndata("mpg", package="gamair", verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight),\n           data = mpg, method = "REML")\n\n# Check overall concurvity\n___\n\n'}},{node:{name:"exc_02_13",code:'library(gamair)\nset.seed(0)\ndata("mpg", package="gamair", verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight),\n           data = mpg, method = "REML")\n\n# Check pairwise concurvity\n___\n\n'}},{node:{name:"solution_02_13",code:'library(gamair)\nset.seed(0)\ndata("mpg", package="gamair", verbose=FALSE)\n\nlibrary(mgcv)\n# Fit the model\nmod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight),\n           data = mpg, method = "REML")\n\n# Check overall concurvity\nconcurvity(mod, full = FALSE)\n'}},{node:{name:"exc_03_02",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Inspect the data\n___\n___\n\n'}},{node:{name:"solution_03_02",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Inspect the data\nhead(meuse)\nstr(meuse)\n'}},{node:{name:"exc_03_02_1",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\nmod2d <- gam(cadmium ~ ___, data = meuse, method = "REML")\n\n# Inspect the model\n---\n---\n'}},{node:{name:"exc_03_05_1",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\n\n# 3D surface plot\n___(mod2da, ___, pages = 1)\n'}},{node:{name:"solution_03_05_1",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\n\n# 3D surface plot\nplot(mod2da, scheme = 1, pages = 1)\n'}},{node:{name:"solution_03_05_2",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\n\n# Colored heat map\nplot(mod2da, scheme = 2, pages = 1)\n'}},{node:{name:"exc_03_05_2",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\ns\n# Colored heat map\nplot(mod2da, ___, ___)\n'}},{node:{name:"exc_03_05",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\n\n# Contour plot\n___(___, pages = 1)\n\n'}},{node:{name:"solution_03_05",code:'library(mgcv)\ndata(meuse, package="sp")\nmod2da <- gam(cadmium ~ s(x, y) + s(dist) + s(elev),\n              data = meuse, method = "REML")\n\n# Contour plot\nplot(mod2da, pages = 1)\n'}},{node:{name:"solution_03_06_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Rotate the same plot\nvis.gam(mod2d, view = c("x", "y"),\n        plot.type = "persp", se = 2, theta = 135)\n'}},{node:{name:"solution_03_06_2",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Rotate the same plot\nvis.gam(mod2d, view = c("x", "y"),\n        plot.type = "persp", se = 2, theta = 135)\n'}},{node:{name:"exc_03_06_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Rotate the same plot\nvis.gam(mod2d, ___)\n'}},{node:{name:"exc_03_06_2",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Rotate the same plot\nvis.gam(mod2d, ___)\n\n'}},{node:{name:"exc_03_07_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 10% extrapolation\nvis.gam(mod2d, view = c("x", "y"), ___, ___)\n\n# Overlay data\npoints(meuse)\n'}},{node:{name:"exc_03_07_2",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 25% extrapolation\nvis.gam(mod2d, view = c("x", "y"),\n        plot.type = "contour", too.far = ___)\n\n# Overlay data\n___\n\n'}},{node:{name:"solution_03_07_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 10% extrapolation\nvis.gam(mod2d, view = c("x", "y"),\n        plot.type = "contour", too.far = 0.1)\n\n# Overlay data\npoints(meuse)\n'}},{node:{name:"solution_03_07_2",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod2d <- gam(cadmium ~ s(x,y), data=meuse, method = "REML")\n\n# Make plot with 25% extrapolation\nvis.gam(mod2d, view = c("x", "y"),\n        plot.type = "contour", too.far = 0.25)\n\n# Overlay data\npoints(meuse)\n'}},{node:{name:"solution_03_09",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit a model with separate smooths for each land-use level\nmod_sep <- gam(copper ~ s(dist, by = landuse) + landuse,\n               data = meuse, method = "REML")\n\n# Examine the summary\nsummary(mod_sep)\n'}},{node:{name:"solution_03_09_1",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit a model with factor-smooth interaction\nmod_fs <- gam(copper ~ s(dist, landuse, bs = "fs"),\n              data = meuse, method = "REML")\n\n# Examine the summary\nsummary(mod_fs)\n'}},{node:{name:"exc_03_09_1",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit a model with a factor-smooth interaction\nmod_fs <- ___\n\n# Examine the summary\n___\n'}},{node:{name:"solution_03_10_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod_sep <- gam(copper ~ s(dist, by=landuse), data=meuse, method = "REML")\nmod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data=meuse, method = "REML")\n\n# Plot both the models with vis.gam()\nvis.gam(mod_sep, view = c("dist", "landuse"), plot.type = "persp")\nvis.gam(mod_fs, view = c("dist", "landuse"), plot.type = "persp")\n'}},{node:{name:"exc_03_10_1",code:'data(meuse, package="sp")\nlibrary(mgcv)\nmod_sep <- gam(copper ~ s(dist, by=landuse), data=meuse, method = "REML")\nmod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data=meuse, method = "REML")\n\n# Plot both the models with vis.gam()\n___(mod_sep, view = c("dist", "landuse"), plot.type = ___)\n___(mod_fs, view = c("dist", "landuse"), plot.type = ___)\n\n'}},{node:{name:"exc_04_02",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nlog_mod <- gam(purchase ~ s(mortgage_age), data = csale,\n               family = binomial,\n               method = "REML")\n\n# Examine the csale data frame\nhead(csale)\nstr(csale)\n\n# Fit a logistic model\nlog_mod <- gam(___, data = csale,\n               ___,\n               method = "REML")\n\n# Calculate the probability at the mean\n___\n'}},{node:{name:"solution_04_02",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nlog_mod <- gam(purchase ~ s(mortgage_age), data = csale,\n               family = binomial,\n               method = "REML")\n\n# Examine the csale data frame\nhead(csale)\nstr(csale)\n\n# Fit a logistic model\nlog_mod <- gam(purchase ~ s(mortgage_age), data = csale,\n               family = binomial,\n               method = "REML")\n\n# Calculate the probability at the mean\nplogis(coef(log_mod)[1])\n'}},{node:{name:"exc_04_03",code:'csale <- readRDS("csale.rds")\nlibrary(mgcv)\n\n# Fit a logistic model\nlog_mod2 <- gam(___,\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# View the summary\n___\n\n'}},{node:{name:"solution_04_03",code:'csale <- readRDS("csale.rds")\nlibrary(mgcv)\n\n# Fit a logistic model\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) +\n                  s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# View the summary\nsummary(log_mod2)\n\n'}},{node:{name:"solution_04_05",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                \t\t   s(avg_prem_balance) + s(retail_crdt_ratio) +\n                           s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n              data = csale,\n              family = binomial,\n              method = "REML")\n\n# Plot on the log-odds scale\nplot(log_mod2, pages = 1)\n'}},{node:{name:"exc_04_05",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot on the log-odds scale\n___\n'}},{node:{name:"exc_04_05_3",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot with intercept uncertainty\nplot(log_mod2, pages = 1, trans = plogis,\n     shift = coef(log_mod2)[1], seWithMean = ___)\n'}},{node:{name:"solution_04_05_1",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot on the probability scale\nplot(log_mod2, pages = 1, trans = plogis)\n'}},{node:{name:"solution_04_05_2",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot with the intercept\nplot(log_mod2, pages = 1, trans = plogis,\n     shift = coef(log_mod2)[1])\n'}},{node:{name:"solution_04_05_3",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot with intercept uncertainty\nplot(log_mod2, pages = 1, trans = plogis,\n     shift = coef(log_mod2)[1], seWithMean = TRUE)\n'}},{node:{name:"exc_04_10",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- ___(log_mod2, newdata = ___,\n                       type = ___, se.fit = ___)\n\n# Inspect the predictions\npredictions\n\n'}},{node:{name:"solution_04_10",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- predict(log_mod2, newdata = new_credit_data,\n                       type = "link", se.fit = TRUE)\n\n# Inspect the predictions\npredictions\n'}},{node:{name:"solution_04_10_1",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- predict(log_mod2, newdata = new_credit_data,\n                       type = "link", se.fit=TRUE)\n\n# Calculate high and low predictions intervals\nhigh_pred <- predictions$fit + 2*predictions$se.fit\nlow_pred <- predictions$fit - 2*predictions$se.fit\n'}},{node:{name:"solution_04_10_2",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- predict(log_mod2, newdata = new_credit_data,\n                       type = "link", se.fit=TRUE)\n\n# Calculate high and low predictions intervals\nhigh_pred <- predictions$fit + 2*predictions$se.fit\nlow_pred <- predictions$fit - 2*predictions$se.fit\n\n# Convert intervals to probability scale\nhigh_prob <- plogis(high_pred)\nlow_prob <- plogis(low_pred)\n\nhigh_prob\nlow_prob\n'}},{node:{name:"exc_04_10_1",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- predict(log_mod2, newdata = new_credit_data,\n                       type = "link", se.fit = TRUE)\n\n# Calculate high and low prediction intervals\nhigh_pred <- ___\nlow_pred <- ___\n'}},{node:{name:"exc_04_10_2",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Calculate predictions and errors\npredictions <- predict(log_mod2, newdata = new_credit_data,\n                       type = "link", se.fit = TRUE)\n\n# Calculate high and low predictions intervals\nhigh_pred <- predictions$fit + 2*predictions$se.fit\nlow_pred <- predictions$fit - 2*predictions$se.fit\n\n# Convert intervals to probability scale\nhigh_prob <- ___\nlow_prob <- ___\n\n# Inspect\nhigh_prob\nlow_prob\n'}},{node:{name:"solution_04_11",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Predict from the model\nprediction_1 <- predict(log_mod2,\n                        newdata = new_credit_data[1, ],\n                        type = "terms")\n\n# Inspect\nprediction_1\n'}},{node:{name:"exc_04_11",code:'library(mgcv)\ncsale <- readRDS("csale.rds")\nnew_credit_data <- readRDS("new_credit_data.rds")\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Predict from the model\nprediction_1 <- ___(log_mod2,\n                        newdata = ___,\n                        type = ___)\n\n# Inspect\nprediction_1\n'}},{node:{name:"exc_04_05_2",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot with the intercept\nplot(log_mod2, pages = 1, trans = plogis,\n     shift = ___)\n'}},{node:{name:"exc_04_05_1",code:'csale <- readRDS("csale.rds")\nset.seed(0)\nlibrary(mgcv)\nlog_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) +\n                  s(avg_prem_balance) + s(retail_crdt_ratio) +\n                  s(avg_fin_balance)  + s(mortgage_age) + s(cred_limit),\n                data = csale,\n                family = binomial,\n                method = "REML")\n\n# Plot on the probability scale\nplot(log_mod2, pages = 1, trans = ___)\n\n\n'}},{node:{name:"solution_03_02_1",code:'library(mgcv)\ndata(meuse, package="sp")\n\n# Fit the model\nmod2d <- gam(cadmium ~ s(x, y), data = meuse, method = "REML")\n\n# Inspect the model\nsummary(mod2d)\ncoef(mod2)\n'}}]}}}},844:function(e){e.exports={data:{allMarkdownRemark:{edges:[{node:{rawMarkdownBody:'\r\n# Multiple Regression with GAMs\r\n\r\nNoam Ross\r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: So far, all the GAMs we have seen have been univariate; they have a single predictor for the outcome. However, we can perform multiple regression with GAMs. Multiple GAMS also can contain a mixture of smooths, linear effects, and continuous or categorical variables. In this lesson, we\'ll learn how to use this flexibility to fit a variety of different models to data.\r\n\r\n---\r\n\r\n# Working Dataset: mpg\r\n\r\n```r\r\nmpg\r\n```\r\n\r\n```out\r\n    symbol loss          make   fuel aspir doors       style drive eng.loc    wb length width\r\n1        3   NA   alfa-romero    gas   std   two convertible   rwd   front  88.6  168.8  64.1\r\n2        3   NA   alfa-romero    gas   std   two convertible   rwd   front  88.6  168.8  64.1\r\n3        1   NA   alfa-romero    gas   std   two   hatchback   rwd   front  94.5  171.2  65.5\r\n4        2  164          audi    gas   std  four       sedan   fwd   front  99.8  176.6  66.2\r\n5        2  164          audi    gas   std  four       sedan   4wd   front  99.4  176.6  66.4\r\n6        2   NA          audi    gas   std   two       sedan   fwd   front  99.8  177.3  66.3\r\n7        1  158          audi    gas   std  four       sedan   fwd   front 105.8  192.7  71.4\r\n8        1   NA          audi    gas   std  four       wagon   fwd   front 105.8  192.7  71.4\r\n9        1  158          audi    gas turbo  four       sedan   fwd   front 105.8  192.7  71.4\r\n10       0   NA          audi    gas turbo   two   hatchback   4wd   front  99.5  178.2  67.9\r\n...\r\n```\r\n\r\nNotes: We\'ll now work with the mpg data set.  This is a data set of 205 models of cars, consisting of various traits like their make, model, cylinders, price and weight, and their city and highway fuel efficiency. We\'ll be building models that use the vehicle traits to predict fuel efficiency.\r\n\r\n---\r\n\r\n# Multiple Smooths (1)\r\n\r\n```r\r\nmodel <- gam(hw.mpg ~ s(weight), data = mpg,\r\n             method = "REML")\r\n```\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/onevar-1.png)\r\n\r\n\r\nNotes: Let\'s start with a very simple one variable model.  Here is the code for a model that predicts highway fuel efficiency as a smooth function of automobile weight. The resulting model captures the nonlinear decreasing relationship between these two variables.\r\n\r\n---\r\n\r\n# Multiple Smooths (2)\r\n\r\n```r\r\nmodel <- gam(hw.mpg ~ s(weight), data = mpg,\r\n             method = "REML")\r\n```\r\n\r\n```r\r\nmodel2 <- gam(hw.mpg ~ s(weight) + s(length), data = mpg,\r\n              method = "REML")\r\n```\r\n\r\n\r\nNotes: To add an additional variable, such as length, we just include\r\nanother s() function in our formula, separated by a plus sign. Here, we add car length as another predictor.\r\n\r\n---\r\n\r\n# Multiple Smooths (3)\r\n\r\n```r\r\nmodel2 <- gam(hw.mpg ~ s(weight) + s(length), data = mpg,\r\n              method = "REML")\r\n```\r\n\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/twovars-1.png)\r\n\r\n\r\nNotes: \r\nWe see from these plots that length has increasing nonlinear effect on fuel economy, and this effect is weaker than the weight effect. \r\n\r\nNote that, in this model, both the effect of weight and price are non-linear terms, but the two are simply added together to get a final prediction.  That addition is where the additive in generalized additive models comes from.\r\n\r\n---\r\n\r\n# Linear terms\r\n\r\n```r\r\nmodel2 <- gam(hw.mpg ~ s(weight) + length, data = mpg,\r\n              method = "REML")\r\n```\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/mixed-linear-1.png)\r\n\r\n\r\nNotes: Not every term in a GAM has to be nonlinear.  You can combine linear and nonlinear terms.  To add a linear term, don\'t wrap the predictor in the s() function.  Here I\'ve made the length term from the previous model linear. \r\n\r\nIn practice, we rarely make continuous variables linear in GAMs. This is because, if the relationship is really linear, or there is not enough data to show otherwise, automatic smoothing will force a linear shape.\r\n\r\n---\r\n\r\n# Linear Terms (2)\r\n\r\n```r\r\nmodel2b <- gam(hw.mpg ~ s(weight) + s(length, sp = 1000), data = mpg,\r\n              method = "REML")\r\n```\r\n\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/mixed-linear-1.png)\r\n\r\nNotes: We can produce the same model if we set the smoothing parameter of the length term very high. Strong smoothing results in a linear model.\r\n\r\n---\r\n\r\n# Categorical Terms (1)\r\n\r\n```r\r\nmodel3 <- gam(hw.mpg ~ s(weight) + fuel, data = mpg,\r\n              method = "REML")\r\n```\r\n\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/categorical-1.png)\r\n\r\nNotes: However, linear terms are very useful when we have categorical variables as predictors.  For instance, in our mpg data set, the fuel variable has two categories: diesel and gas. When we include a linear term with this categorical variable, the gam() function fits a model with a fixed effect for each level of the category. Here, you can see that having gasoline has a negative effect; diesel vehicles are more fuel efficient. \r\n\r\nIn this model the nonlinear effect of weight applies to vehicles of both gas types. This is nonlinear equivalent to the fixed slope, varying intercept models you may recall from a linear regression course.\r\n\r\nNote that, when you use categorical variables this way, it\'s important that the variables are stored as factors. The mgcv package does not use character variables.\r\n\r\n---\r\n\r\n# Categorical Terms (2)\r\n\r\n```r\r\nmodel4 <- gam(hw.mpg ~ s(weight, by = fuel), data = mpg,\r\n              method = "REML")\r\n```\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/categorical-by-1.png)\r\n\r\n\r\nNotes: We can also specify a GAM formula that will fit different smooths for different categorical variables.  We call this a factor-smooth interaction.  By specifying the "by" argument to the s() function, we can tell R to calculate a different smooth for each unique category.\r\n\r\nYou can see here we have different smooths for diesel and gas cars, but the diesel smooth is much more uncertain.\r\n\r\n---\r\n\r\n# Categorical Terms (3)\r\n\r\n```r\r\nmodel4b <- gam(hw.mpg ~ s(weight, by = fuel) + fuel, data = mpg,\r\n              method = "REML")\r\n```\r\n\r\n![](http://s3.amazonaws.com/assets.datacamp.com/production/course_6790/datasets/categorical-by-intercept-1.png)\r\n\r\n\r\nNotes: Usually, when we have smooth-factor interactions, we want to also include a varying intercept, in case the different categories are different in overall means in addition to shape of their smooths.  Here, you see adding this varying intercept improves the estimate of the smooth for diesel cars.\r\n\r\n---\r\n\r\n# Let\'s practice!\r\n\r\nNotes: Now, let\'s practice fitting these different combinations of models.\r\n\r\n\r\n\r\n\r\n\r\n',fields:{slug:"/chapter1_09"}}},{node:{rawMarkdownBody:"\r\n# Introduction to Generalized Additive Models\r\n\r\nNoam Ross \r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Hi, I'm Noam Ross. I'm a scientist who studies infectious diseases. I use R and Generalized Additive Models to better understand complex biological and ecological systems. This course will teach you how to use these versatile models to analyze and understand complex, multifaceted, non-linear relationships in your own work.\r\n\r\n---\r\n\r\n# Trade-offs in Model Building\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/tradeoff-slider.png?raw=true)\r\n\r\n\r\nNotes: Whenever we build statistical models, we face a trade-off between flexibility and interpretability. GAMs offer a middle ground between simple models, such as those we fit with linear regression, and more complex machine learning models like neural networks.\r\n\r\nLinear models are easy to interpret and to use for inference: It is easy to understand the meaning of their parameters. However, we often need to model more complex phenomena than can be represented by linear relationships.\r\n\r\nOn the other hand, machine learning models, like boosted regression trees or neural networks, can be very good at making predictions of complex relationships. The problem is that they tend to need lots of data, are quite difficult to interpret, and one can rarely make inferences from the model results.\r\n\r\nGAMs offer a middle ground: they can be fit to complex, nonlinear relationships and make good predictions in these cases, but we are still able to do inferential statistics and understand and explain the underlying structure of our models and why they make predictions that they do.\r\n\r\n---\r\n\r\n# Non-linear Relationships\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/nonlinear-scatter-1.png?raw=true)\r\n\r\nNotes: GAMs let us flexibly model non-linear relationships. Here I've made a scatter plot of two variables, X and Y. We can see from the scatterplot that there is clearly some relationship between the variables, but it is not linear.\r\n\r\n---\r\n\r\n# Non-linear Relationships (2)\r\n\r\n```r\r\nlinear_mod <- lm(y ~ x, data = my_data)\r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/nonlinear-scatter-lm-1.png?raw=true)\r\n\r\nNotes: If we fit a linear model to the data using the lm() function and the usual formula syntax, we can see it won't do a very good job. The model doesn't capture key aspects of this relationship.\r\n\r\n---\r\n\r\n# Non-linear Relationships (3)\r\n\r\n```{r}\r\nlibrary(mgcv)\r\ngam_mod <- gam(y ~ s(x), data = my_data)\r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/nonlinear-scatter-gam-1.png?raw=true)\r\n\r\nNotes: With a GAM, however, we can fit data with smooths, or splines, which are functions that can take on a wide variety of shapes. \r\nWe fit a GAM using the gam() function from the mgcv package. \r\nHere, when we fit this GAM, we wrap the independent variable, x, in the s(), that is smooth function to specify that we want this relationship to be flexible.\r\nA GAM can capture the nonlinear aspects of not only this relationship, but of many nonlinear relationships, because of the flexibility of splines.\r\n\r\n---\r\n\r\n# Basis Functions\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/basis-functions-1.png?raw=true)\r\n\r\nNotes: The flexible smooths in GAMs are actually constructed of many smaller functions. These are called basis functions. Each smooth is the sum of a number of basis functions, and each basis function is multiplied by a coefficient, each of which is a parameter in the model. In the plot here on the left, we show the basis functions of a GAM where all the coefficients are the same. On the right, we show the same basis functions after model-fitting, where each has a coefficient fit to the data. You can see how these basis functions add up to create the overall smooth shape. So a single nonlinear relationship between a dependent and independent variable has several parameters, plus an intercept. This is different, and more complex, than a linear model, where each variable has only a single coefficient or parameter.\r\n\r\n---\r\n\r\n# Basis Functions (2)\r\n\r\n```{r}\r\ngam_mod <- gam(y ~ s(x), data = my_data)\r\n\r\ncoef(gam_mod)\r\n\r\n(Intercept)     s(x2).1     s(x2).2    \r\n7.814448        5.272290    5.104941\r\n     \r\ns(x2).3         s(x2).4     s(x2).5\r\n1.271135        1.720561   -1.180613 \r\n     \r\ns(x2).6 \r\n-2.676133\r\n```\r\n\r\nNotes: When we fit a GAM with R, we can extract the coefficients just like we can for linear models using the coef() function.  Calling this function on a GAM model object will show the coefficients of each of the basis functions of the model.  You can see that even a simple one-smooth model has many coefficients.\r\n\r\n---\r\n\r\n# Let's practice!\r\n\r\nNotes: Now, let's fit our first set of models.\r\n\r\n\r\n",fields:{slug:"/chapter1_01_introduction"}}},{node:{rawMarkdownBody:"\r\n# Interpreting GAM outputs\r\n\r\nNoam Ross \r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: So far, we've learned how to fit generalized additive models to data.  \r\n\r\nNow we'll take a more detailed look at model outputs, to learn how to interpret the results of our model-fitting and better understand the relationships between variables.\r\n\r\n---\r\n\r\n# GAM Summaries\r\n\r\n```r\r\nmod_hwy <- gam(hw.mpg ~ s(weight) + s(rpm) + \r\n               s(price) + s(comp.ratio) +\r\n               s(width) + fuel + cylinders,\r\n               data = mpg, method = \"REML\")\r\n```\r\n\r\n```r\r\nsummary(mod_hwy)\r\n```\r\nNotes: As with other models in R, you can get a summary of model statistics with the summary() function.  Let's look at this output for a model we fit with the mpg data.\r\n\r\n---\r\n\r\n# GAM Summaries (2)\r\n\r\n```r\r\nsummary(mod_hwy)\r\n```\r\n\r\n```out    \r\nFamily: gaussian\r\nLink function: identity\r\n\r\nFormula:\r\nhw.mpg ~ s(weight) + s(rpm) + s(price) + s(comp.ratio) +\r\n  s(width) + fuel\r\n    \r\nParametric coefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   23.873      3.531   6.760 1.89e-10 ***\r\nfuelgas        7.571      3.922   1.931   0.0551 .\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n    \r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***\r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***\r\ns(price)      2.681  3.421  1.678    0.155\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***\r\ns(width)      1.001  1.001  0.357    0.551\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n    \r\nR-sq.(adj) =   0.89   Deviance explained = 90.1%\r\nREML = 464.81  Scale est. = 5.171     n = 199\r\n```\r\nNotes: There's a lot here, so let's go through it piece by piece.\r\n\r\n---\r\n\r\n# GAM Summaries (3)\r\n\r\n```r\r\nsummary(mod_hwy)\r\n```\r\n\r\n```out\r\nFamily: gaussian\r\nLink function: identity\r\n\r\nFormula:\r\nhw.mpg ~ s(weight) + s(rpm) + s(price) +\r\n  s(comp.ratio) + s(width) + fuel\r\n```\r\n\r\nNotes: The first part of the summary describes the model we fit. The \"Family\" component tells us the model assumes a Gaussian or normal distribution of our errors, and the \"Link\" of \"identity\" shows that the model doesn't transform the predictions. We'll talk more about transformation in the final chapter of the course. \r\n\r\nAfter that, we have the model formula.\r\n\r\n---\r\n\r\n# GAM Summaries (4)\r\n\r\n```r\r\nsummary(mod_hwy)\r\n``` \r\n\r\n```out    \r\nParametric coefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   23.873      3.531   6.760 1.89e-10 ***\r\nfuelgas        7.571      3.922   1.931   0.0551 .\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '\r\n```\r\n\r\nNotes: The next section describes the parametric terms of our model. Parametric means models that have a pre-determined form.  In this context, it refers to the linear terms in the model. \r\n\r\nThis section may be familiar from linear modeling. It shows the coefficients for the linear terms in the model, their values, errors, test statistics, and p-values.  Asterisks next to the p-values indicate statistical significance. In this case, the model intercept is significant, but the fixed effect of fuel type is only significant at the 0.1 level.\r\n\r\n---\r\n\r\n# GAM Summaries (5)\r\n\r\n```r\r\nsummary(mod_hwy)\r\n```  \r\n    \r\n```out\r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***\r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***\r\ns(price)      2.681  3.421  1.678    0.155\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***\r\ns(width)      1.001  1.001  0.357    0.551\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n```\r\nNotes: The next section covers smooth terms.  For smooths coefficients are not printed.  This is because each smooth has several coefficients - one for each basis function.  Instead, the first column reads edf, which stands for effective degrees of freedom.  This value represents the complexity of the smooth.  An edf of 1 is equivalent to a straight line.  An edf of 2 is equivalent to a quadratic curve, and so on, with higher edfs describing more wiggly curves.\r\n\r\n---\r\n\r\n# Effective Degrees of Freedom\r\n\r\n```out\r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***  <--\r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***\r\ns(price)      2.681  3.421  1.678    0.155\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***  <--\r\ns(width)      1.001  1.001  0.357    0.551      \r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/edf-1.png?raw=true)\r\n\r\n\r\nNotes: We can see this by plotting.  Here are partial effect plots for the weight and compression ratio smooths. You can see the weight smooth, with an edf over 6, is complex and wiggly.  But the compression ratio smooth, with an edf of 1, is linear.\r\n\r\n---\r\n\r\n# Significance of Smooth Terms\r\n\r\n```out\r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***\r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***\r\ns(price)      2.681  3.421  1.678    0.155\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***\r\ns(width)      1.001  1.001  0.357    0.551\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n```\r\n\r\nNotes: The terms to the right of the EDF column have to do with significance testing for smooths. The Ref.df and F columns are test statistics used in an ANOVA test to test overall significance of the smooth.  The result of this test is the p-value to the right.  It's important to note that these values are approximate, and it's important to visualize your model to check them.\r\n\r\n---\r\n\r\n# Significance of Smooth Terms (2)\r\n\r\n```out\r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***  <--\r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***  \r\ns(price)      2.681  3.421  1.678    0.155      <--\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***\r\ns(width)      1.001  1.001  0.357    0.551      \r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/signif-1.png?raw=true)\r\n\r\nNotes: A good way to interpret significance for smooth terms in GAMs is this: a significant smooth term is one where you can not draw a horizontal line through the 95% confidence interval.\r\n\r\nIf we look at plots of the weight and price smooths, we see this. Clearly a horizontal line can't go through the weight smooth confidence interval, but a horizontal line just fits inside the price smooth interval.\r\n\r\n---\r\n\r\n# Significance and Effective Degress of Freedom\r\n\r\n```out\r\nApproximate significance of smooth terms:\r\n                edf Ref.df      F  p-value\r\ns(weight)     6.254  7.439 20.909  < 2e-16 ***  \r\ns(rpm)        7.499  8.285  8.534 2.07e-09 ***  \r\ns(price)      2.681  3.421  1.678    0.155      <--\r\ns(comp.ratio) 1.000  1.001 18.923 2.22e-05 ***  <--\r\ns(width)      1.001  1.001  0.357    0.551      <--\r\n```\r\n    \r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/signif2-1.png?raw=true)\r\n\r\nNotes: Note that high EDF doesn't mean significance or vice-versa. A smooth may be linear and significant, non-linear and non-significant, or one of each.  \r\n\r\nIn this model, the price term is non-linear but non-significant, meaning it has some complexity but there isn't certainty as to the shape or direction of its effect.  Compression ratio is linear but significant.  Width is neither.\r\n\r\n---\r\n\r\n# Let's practice!\r\n\r\nNotes: Now, let's take a close look at some of your models.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",fields:{slug:"/chapter2_01"}}},{node:{rawMarkdownBody:"\n# Visualizing GAMs\n\nNoam Ross\nSenior Research Scientist, EcoHealth Alliance\n\nNotes: One of the most important things to do when interpreting and checking models is to visualize them.  This allows us to inspect them and gain intuition for the model relationships. And of course, visualizations are often the most powerful way to communicate our results.\n\n---\n\n# The Plot Command\n\n```r\nplot(gam_model)\n```\n\n```r\n?plot.gam\n```\nNotes: We have already been using the central function for visualizing GAMs, plot(). The mgcv package has a powerful set of plot methods built into this function. We will go through options for plotting now, and you can also look them up in R help under ?plot.gam.\n\n---\n\n# Partial Effect Plots\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/partial-effects-1.png?raw=true)\n\nNotes: The plots generated by mgcv's plot() function are partial effect plots.  That is, they show the component effect of each of the smooth or linear terms in the model, which add up to the overall prediction.\n\n---\n\n# Selecting partial effects\n\n```r\nplot(gam_model, select = c(2, 3))\nplot(gam_model, pages = 1)\nplot(gam_model, pages = 1, all.terms = TRUE)\n```\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/all-terms-1.png?raw=true)\n\nNotes: The first option we have when making our plots is which partial effects to show.  The select argument chooses which terms we plot, with the default being all of them. \n\nNormally, each plot gets its own page, but using the pages argument, you can decide how many total pages to spread plots across. Using pages = 1, you show all your partial effects together.\n\nFinally, by default we only see the smooth plots, but by setting all.terms = TRUE, we can display partial effects of linear or categorical terms, as well.\n\n---\n\n# Showing data on the plots\n\n```{r}\nplot(gam_model, rug = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/showing-data-1.png?raw=true)\n\nNotes: We often want to show data alongside model predictions.  There are two ways to do this. First, the rug argument puts X-values along the bottom of the plot.\n\n---\n\n# Showing data on the plots (2)\n\n```{r}\nplot(gam_model, residuals = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/showing-data-2.png?raw=true)\n\nNotes: The residuals argument puts partial residuals on the plots. Partial residuals are the difference between the partial effect and the data, after all other partial effects have been accounted for.\n\n\n---\n\n# Showing data on the plots (3)\n\n```{r}\nplot(gam_model, rug = TRUE, residuals = TRUE,\n     pch = 1, cex = 1)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/showing-data-3.png?raw=true)\n\nNotes: It's often helpful to make these more visible with the pch argument, which changes the shape of the residuals points and the cex argument, which changes the size.\n\n---\n\n# Showing Standard Errors\n\n```{r}\nplot(gam_model, se = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/shading-se-1.png?raw=true)\n\nNotes: By default, plot will put standard errors on your plots. These show the 95% confidence interval for the mean shape of the effect.\n\n---\n\n# Showing Standard Errors (2)\n\n```{r}\nplot(gam_model, shade = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/shading-se-2.png?raw=true)\n\nNotes: It's often preferable to use shading rather than lines to show these intervals, and you can do so with the shade argument.\n\n---\n\n# Showing Standard Errors\n\n```{r}\nplot(gam_model, shade = TRUE, shade.col = \"lightblue\")\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/shading-se-3.png?raw=true)\n\nNotes: You can also change the color of shading with the shade.col argument.\n\n---\n\n# Transforming Standard Errors\n\n```{r}\nplot(gam_model, seWithMean = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/se-with-mean-1.png?raw=true)\n\nNotes: It's often useful to plot the standard errors of a partial effect term combined with the standard errors of the model intercept.  This is because confidence intervals at the mean value of a variable can be very tiny, and don't reflect overall uncertainty in our model.  Using the seWithMean argument adds in this uncertainty.\n\n---\n\n# Transforming Standard Errors (2)\n\n```{r}\nplot(gam_model, seWithMean = TRUE, shift = coef(gam_model)[1])\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/se-with-mean-2.png?raw=true)\n\nNotes: To make the plots even more interpretable, it's useful to shift the scale so that the intercept is included. Using the shift argument, we can shift the scale by value of the intercept, which is the first coefficient of the model.  Note how the y-axis has changed. Now, the partial effect plot has a more natural interpretation - it shows us the prediction of the output, assuming other variables are at their average value.  For instance, this plot shows that the miles per gallon of a 2000 pound car is about 30, all else being equal.\n\n---\n\n# Now lets make some plots!\n\nNotes: Now let's make some plots!\n\n\n\n\n\n\n\n\n\n",fields:{slug:"/chapter2_04"}}},{node:{rawMarkdownBody:"\r\n# Model checking with gam.check()\r\n\r\nNoam Ross\r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Now that we can fit and plot GAMs, we need some checks to make sure that we have well-fit models. There are several pitfalls we need to look out for when fitting GAMs. Thankfully, mgcv provides helpful tools to check for these.\r\n\r\n---\r\n\r\n# Pitfall One: Inadequate Basis Number\r\n\r\n```r\r\nmod <- gam(y ~ s(x1, k = 4) + s(x2, k = 4),\r\n           data = check_data, method = \"REML\")\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/fourbases-1.png?raw=true)\r\n\r\n\r\nNotes: We've learned that the number of basis functions determines how wiggly a smooth can be.  If there are not enough basis functions, it may not be wiggly enough to capture the relationships in data.  Here is a model where smooths have 4 basis functions.  As we see in this partial effect plot, this is not enough to capture the pattern.\r\n\r\nIt's not always obvious visually whether we have enough basis functions.  We can test for this, though, via the gam.check() function.\r\n\r\n---\r\n\r\n# # Running `gam.check`\r\n\r\n```r\r\ngam.check(mod)\r\n```\r\n\r\n```out\r\nMethod: REML   Optimizer: outer newton\r\nfull convergence after 9 iterations.\r\nGradient range [-0.0001467222,0.00171085]\r\n(score 784.6012 & scale 2.868607).\r\nHessian positive definite, eigenvalue range [0.00014,198.5]\r\nModel rank =  7 / 7 \r\n\r\nBasis dimension (k) checking results. Low p-value\r\n(k-index<1) may indicate that k is too low, especially\r\nif edf is close to k'.\r\n\r\n        k'  edf k-index p-value    \r\ns(x1) 3.00 1.00    0.35  <2e-16 ***\r\ns(x2) 3.00 2.88    1.00    0.52    \r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1\r\n```\r\n\r\nNotes: Running gam.check() on a model provides several outputs, in both the console and as plots.  We'll start with the console output.\r\n\r\nFirst, gam.check() reports on model convergence.  Here, it reports full convergence. R has found a best solution.  If the model has not converged,  results are likely not correct. This can happen when there are too many parameters in the model for not enough data.\r\n\r\nBelow, we see a table of basis checking results. This shows a statistical test for patterns in model residuals, which should be random.  Each line reports the test results for one smooth. It shows the k value or number of basis functions, the effective degrees of freedom, a test statistic, and p-value.\r\n\r\nHere, small p-values indicate that residuals are not randomly distributed. This often means there are not enough basis functions. \r\n\r\nThis is an approximate test. Always visualize your results too, and compare the k and edf values in addition to looking at the p-value.\r\n\r\n---\r\n\r\n# Running gam.check (2)\r\n\r\n```r\r\nmod <- gam(y ~ s(x1, k = 12) + s(x2, k = 4),\r\n                data = dat, method = \"REML\")\r\ngam.check(mod)\r\n```\r\n    \r\n```out\r\n...\r\n    \r\n         k'   edf k-index p-value  \r\ns(x1) 11.00 10.85    1.05   0.830  \r\ns(x2)  3.00  2.98    0.89   0.015 *\r\n    \r\n...\r\n```\r\n\r\nNotes: If we re-fit our model with higher k , we see that this test  is no longer significant. However, now we see a problem with the second smooth - the p-value for its test is now significant.  Fixing one problem can reveal another.  So it is always important to re-run gam.check after changing models.\r\n\r\n---\r\n\r\n# Running `gam.check` (3)\r\n\r\n```r\r\nmod <- gam(y ~ s(x1, k = 12) + s(x2, k = 12),\r\n            data = dat, method = \"REML\")\r\ngam.check(mod)\r\n```\r\n\r\n```out \r\n...\r\n         k'   edf k-index p-value\r\ns(x1) 11.00 10.86    1.08    0.94\r\ns(x2) 11.00  7.78    0.94    0.12\r\n...\r\n```\r\n\r\n\r\nNotes: If we increase the k value for the second smooth, and run gam.check() again, now both smooths pass the test. \r\n\r\nNeither has significant patterns in their residuals and both have enough basis functions.\r\n\r\n---\r\n\r\n# Diagnostic Plots\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/gamcheck-1.png?raw=true)\r\n\r\nNotes: gam.check() will also generate four plots. Each of these gives a different way of looking at your model residuals.  These plots show the results from the original, poorly fit model. On the top-left is a Q-Q plot, which compares the model residuals to a normal distribution.  A well-fit model's residuals will be close to a straight line.  On bottom left is a histogram of residuals.  We would expect this to have a symmetrical bell shape.  On top-right is a plot of residual values.  These should be evenly distributed around zero.  Finally, on the bottom-right is plot of response against fitted values.  A perfect model would form a straight line.  We don't expect a perfect model, but we do expect the pattern to cluster around the 1-to-1 line.\r\n\r\n---\r\n\r\n# Diagnostic Plots\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/gamcheck2-1.png?raw=true)\r\n\r\nNotes: Now, here is the output of our final model, with larger k values.  See that the Q-Q plot no longer curves, the histogram is bell shaped, and the comparison of response vs. fitted values clusters around a 1-to-1 line.  These all indicate a much better model fit.\r\n\r\n---\r\n\r\n# Let's check some models\r\n\r\nNotes: Now let's see how well some of the other models fit.\r\n\r\n",fields:{slug:"/chapter2_08"}}},{node:{rawMarkdownBody:'\r\n# Plotting GAM interactions\r\n\r\nNoam Ross\r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Interactions in GAMs are powerful tools for modeling complex or spatial data.  However, their complexity makes them challenging to understand.  Visualizing interactions helps us unlock understanding of these complexities. In this lesson we will learn mgcv\'s tools for visualizing interactions in two and three dimensions.\r\n\r\n---\r\n\r\n# Using mgcv\'s `plot()` command with interactions.\r\n\r\n```r\r\nplot(mod_2d)\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/contourA-1.png?raw=true)\r\n\r\nNotes: In mgcv\'s plot() command,  interactions are represented with contour plots.  A single plot represents both variables and their interaction. In this plot the axes represent values of our predictor variables, x1 and x2.  The interior is a topographic map of predicted values.  The contour lines represent points of equal predicted values, and they are labeled.  The dotted lines show uncertainty in prediction; they represent how contour lines would move if predictions were one standard error higher or lower.\r\n\r\n---\r\n\r\n# Using mgcv\'s `plot()` with interactions\r\n\r\n```r\r\nplot(mod_2d, scheme = 1)\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/contourB-1.png?raw=true)\r\n\r\n\r\nNotes: A contour plot is not always the most intuitive way of plotting interactions, so mgcv has a couple of more options.  Setting the scheme parameter in the plot function to 1, we get a 3D perspective plot instead.\r\n\r\n---\r\n\r\n# Using mgcv\'s `plot()` with interactions\r\n\r\n```r\r\nplot(mod_2d, scheme = 2)\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/contourC-1.png?raw=true)\r\n\r\nNotes: Setting scheme to 2 generates a heat map. This is a simplified contour map on top of which colors are added.  The yellow colors represent larger predictions and the red colors smaller ones.\r\n\r\n---\r\n\r\n# Customizing interaction plots with `vis.gam()`\r\n\r\n```r\r\nvis.gam(x,\r\n        view = NULL,\r\n        cond = list(),\r\n        n.grid = 30,\r\n        too.far = 0,\r\n        col = NA,\r\n        color = "heat",\r\n        contour.col = NULL,\r\n        se = -1,\r\n        type = "link",\r\n        plot.type = "persp",\r\n        zlim = NULL,\r\n        nCol = 50,\r\n        ...)\r\n```\r\n\r\nNotes: These pre-defined schemes are a nice way to view your model interactions quickly.  However, we often want to customize these plots.  For this, we can use mgcv\'s vis.gam().  vis.gam() has MANY options. We will focus on a few important ones.\r\n\r\n---\r\n\r\n# Customizing interaction plots with `vis.gam()`\r\n\r\n```r\r\nvis.gam(x = mod,                # GAM object\r\n        view = c("x1", "x2"),   # variables\r\n        plot.type = "persp")    # kind of plot \r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/twoperspB-1.png?raw=true)\r\n\r\n\r\nNotes: The first argument to vis.gam(), x, is the GAM model.  The second, view, is where you list which variables in the model you want to visualize jointly.  \r\n\r\nSetting the plot.type argument to "persp" will produce a 3D perspective plot as shown here.\r\n\r\nIn this case, x1 and x2 are interacting variables, but they do not need to be.  You can view a perspective plot of any two variables in the model to see their combined effects.\r\n\r\n\r\n---\r\n\r\n# Customizing interaction plots with `vis.gam()` (2)\r\n\r\n```r\r\nvis.gam(x = mod,                # GAM object\r\n        view = c("x1", "x2"),   # variables\r\n        plot.type = "contour")  # kind of plot \r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/twoperspB-2.png?raw=true)\r\n\r\nNotes: Or you can set plot.type to "contour", which will produce a contour plot or heat map.\r\n\r\n---\r\n\r\n# Customizing interaction plots with `vis.gam()`\r\n\r\n```r\r\nvis.gam(mod, view = c("x1", "x2"), plot.type = "contour", too.far = 0.1)\r\nvis.gam(mod, view = c("x1", "x2"), plot.type = "contour", too.far = 0.05)\r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/toofar2-1.png?raw=true)\r\n\r\nNotes: The too.far argument is an important one in using these plots to inspect your model.  too.far indicates what predictions should not be plotted because they are too far from the actual data.  In other words, how far is too far to extrapolate? Setting this value lets you see what combinations of variables are not represented in your data and therefore might not yield good predictions in your model.   \r\n\r\ntoo.far is scaled from zero to one along the range of the variables.  Here, we set it at 0.1 for the left plot and 0.05 on the right. On the left, 10% extrapolation fills in most of the surface. On the right, 5% extrapolation shows more areas not supported by data.\r\n\r\n---\r\n\r\n# Options for perspective plots\r\n\r\n```{r}\r\nvis.gam(x = mod, view = c("x1", "x2"), \r\n        plot.type = "persp", se = 2)          \r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/se-1.png?raw=true)\r\n\r\nNotes: For perspective plots, one can display confidence intervals of the predictions with the "se" argument.  se takes a number which is the number of standard errors away from the average prediction to plot high- and low-prediction surfaces.\r\n\r\n---\r\n\r\n# Options for perspective plots\r\n\r\n```{r}\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "persp", theta = 220)\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "persp", phi = 55)\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "persp", r = 0.1)\r\n```\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/persp3d-1.png?raw=true)\r\n\r\nNotes: You can also control the rotation angle of your perspective plots.  The theta parameter controls horizontal rotation, the phi parameter controls vertical rotation, and the r parameter controls how zoomed in the plot is.  Plots with low r values have lots of distortion or parallax, while plots with high r values have little sense of perspective.\r\n\r\n---\r\n\r\n# Options for contour plots\r\n\r\n```{r}\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "contour", color = "gray")\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "contour", contour.col = "blue")\r\nvis.gam(g, view = c("x1", "x2"), plot.type = "contour", nlevels = 20)\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/contouropts-1.png?raw=true)\r\n\r\nNotes: When making contour plots, color and contrast are important for clarity.  Three important additional options for contours shown here are the color parameter, which selects the background color palette, the contour.col parameter, which selects the color of lines, and the nlevels option, which lets you control the number of contour lines - important for showing details and subtleties of interactions.\r\n\r\n---\r\n\r\n# Now let\'s make some plots!\r\n\r\nNotes: Now let\'s make some plots!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',fields:{slug:"/chapter3_04"}}},{node:{rawMarkdownBody:"\r\n# Visualizing categorical-continuous interactions\r\n\r\nNoam Ross\r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Simple smooth x,y interactions are not the only kind of complex relationship between variables in GAMs. Let's take a look at how to visualize another type of interaction.\r\n\r\n---\r\n\r\n# Categorical-continuous interactions\r\n\r\n```r\r\nmodel4b <- gam(hw.mpg ~ s(weight, by = fuel) + fuel, data = mpg,\r\n              method = \"REML\")\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/categorical-by-intercept-1.png?raw=true)\r\n\r\n\r\nNotes: Back in Chapter 1, we introduced the idea of factor-smooth interactions.  That is, models where we used the \"by\" argument in a smooth to fit different smooths for each value of a categorical variable.  Here's the fuel economy model we fit, where we used different smooths for the effect of weight on fuel economy depending on the fuel type used by cars.\r\n\r\n---\r\n\r\n# Factor-smooths\r\n\r\n```r\r\nmodel4c <- gam(hw.mpg ~ s(weight, fuel, bs = \"fs\"),\r\n               data = mpg,\r\n               method = \"REML\")\r\n```\r\n\r\nNotes: There's another kind of categorical-continuous interaction, called a \"factor-smooth\".  In a factor-smooth, rather than using the by argument, we use a factor-smooth basis type.  We specify the two variables as part of the smooth by setting the bs argument to \"fs\".  \r\n\r\nNote that in this case, we do not include an additional linear term to make separate intercepts for each level.  The factor-smooth formulation accounts for this automatically.\r\n\r\n---\r\n\r\n# Factor-smooths\r\n\r\n```out\r\n> summary(model4c)\r\n\r\nFamily: gaussian \r\nLink function: identity \r\n\r\nFormula:\r\nhw.mpg ~ s(weight, fuel, bs = \"fs\")\r\n\r\nParametric coefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   28.644      7.615   3.761 0.000223 ***\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nApproximate significance of smooth terms:\r\n                edf Ref.df     F p-value    \r\ns(weight,fuel) 7.71     19 53.12  <2e-16 ***\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nR-sq.(adj) =  0.832   Deviance explained = 83.8%\r\n-REML = 518.54  Scale est. = 7.9735    n = 205\r\n```\r\n\r\nNotes: With factor-smooths, we do not get a different term for each level of the categorical variable.  Rather, we get one overall interaction term.  This means that they are not as good for distinguishing between categories.  However, factor smooths are good for controlling for the effects of categories that are not our main variables of interest, especially when there are very many categories, or only a few data points in some categories.\r\n\r\n---\r\n\r\n# Plotting factor-smooths\r\n\r\n```r\r\nplot(model4c)\r\nvis.gam(model4c, theta = 125, plot.type = \"persp\")\r\n```\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/plotvvisgam-1.png?raw=true)\r\n\r\n\r\nNotes: When you call the plot() function on a GAM with a factor-smooth fit using the bs=\"fs\" argument, it will, by default, make one plot with multiple smooths on it.  \r\n\r\nHowever, you can also use the vis.gam() function to visualize factor-smooths.  These staircase-like perspective plots are often helpful for comparing the shapes of different smooths.\r\n\r\n---\r\n\r\n#  Let's practice!\r\n\r\nNotes: Now lets explore different categorical-continuous interactions with the meuse data set.\r\n\r\n\r\n",fields:{slug:"/chapter3_08"}}},{node:{rawMarkdownBody:"\r\n# Logistic GAMs for Classification\r\n\r\nNoam Ross \r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Up until now, we have been using GAMs to model only one type of outcome - continuous numeric values. However, GAMs have the ability to model many other types of outcomes.  In this chapter, we'll learn how to use logistic GAMs for binary classification.\r\n---\r\n\r\n# Types of outcomes\r\n\r\n**Continuous outcomes**\r\n\r\n- Speed of a motorcycle (mph)\r\n- Fuel efficiency of a car (mpg)\r\n- Level of pollution in soil (g/kg)\r\n\r\n**Binary outcomes** \r\n\r\n- Presence or absence of an organism in a location \r\n- Whether a purchase was made \r\n- Yes/No answer on a survey \r\n\r\nNotes: Our previous data had all outcomes, or Y values, that could take on many different numeric values, such as speed, fuel efficiency or concentration of pollution.\r\n\r\nHowever, we often want to model data with binary outcomes, like the presence of organisms, customer conversion, or yes/no answers on a survey.  We need to modify our models to take into account this type of data.\r\n\r\n---\r\n\r\n# Probabilities and Log-Odds: Logistic Function\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/logistic-1.png?raw=true)\r\n\r\nNotes: When we model a binary outcome, our prediction is a probability, which must be between zero and one. Since GAMs can have an outcome of any number, we convert the GAM output to a probability using a logistic function. The logistic function is a transformation that converts numbers of any value to probabilities between zero and one.  In this context the numbers that take on any value can be interpreted as log-odds - the log of the ratio of positive outcomes to negative outcomes.\r\n\r\n---\r\n\r\n# Probabilities and Log-Odds: Logit Function\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/logit-1.png?raw=true)\r\n\r\nNotes: The inverse of the logistic function is the logit function, which translates probabilities between zero and one to log-odds which can have any value.\r\n\r\n---\r\n\r\n# Logistic and Logit Functions in R\r\n\r\n```r\r\nplogis() # Logistic\r\nqlogis() # Logit\r\n\r\nqlogis(plogis(0.5))\r\n[1] 0.5\r\n\r\nqlogis(0.25) == log(1/3)\r\n[1] TRUE\r\n```\r\n\r\nNotes: In R, the logistic function is plogis(), and the logit function is qlogis(). These functions are inverses of each other.  As you can see, the logistic of a logit returns the original value.  You can also see how probabilities convert to odds. A 0.25 probability converts to log-odds by taking a log of the ratio of positives - one - to negatives - three.\r\n\r\n---\r\n\r\n# Logistic GAMs with mgcv\r\n\r\n```r\r\ngam(y ~ x1 + s(x2),\r\n   data = dat,\r\n   family = binomial,\r\n   method = \"REML\")\r\n```\r\n\r\nNotes: To use logistic and logit functions to fit a GAM where we have binary outcomes, we add the family=binomial argument to our GAM function call.  This tells the GAM function that outcomes are ones and zeros, and that it should fit the model on a logistic scale.\r\n\r\n---\r\n\r\n# Logistic GAM outputs\r\n\r\n```out\r\nFamily: binomial \r\nLink function: logit \r\n\r\nFormula:\r\ny ~ s(x1) + s(x2)\r\n\r\nParametric coefficients:\r\n            Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)   0.7330     0.1208    6.07 1.28e-09 ***\r\n ---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nApproximate significance of smooth terms:\r\n        edf Ref.df Chi.sq  p-value    \r\ns(x1) 1.367  1.646  25.83 1.23e-05 ***\r\ns(x2) 5.754  6.890  51.37 8.12e-09 ***\r\n```\r\n\r\n```r\r\nplogis(0.733)\r\n[1] 0.6754633\r\n```\r\n\r\nNotes: The output of a logistic GAM looks similar to that of previous GAMs we fit. As with regular GAMs, non-parametric terms are on top, and smooths on the bottom. EDFs still indicate the complexity of smooths, and asterisks indicate significance.  However, it's important to understand that outputs are on the log-odds scale. To interpret them as probabilities, we need to convert them using the logistic function.  Here, the value of the intercept is 0.733.  We can use the plogis() logistic function to convert it to a probability.\r\n\r\nConverted, the intercept is about 0.67.\r\n\r\nThis means that the model predicts a 67 percent baseline chance of a positive outcome. This is what we would expect if x1 and x2 were at their average values.\r\n\r\n---\r\n\r\n# The `csale` data set\r\n\r\n```r\r\nhead(csale)\r\n```\r\n\r\n```out\r\n  purchase n_acts bal_crdt_ratio avg_prem_balance retail_crdt_ratio\r\n1        0     11        0.00000         2494.414           0.00000\r\n2        0      0       36.09506         2494.414          11.49123\r\n3        0      6       17.60000         2494.414           0.00000\r\n4        0      8       12.50000         2494.414           0.80000\r\n5        0      8       59.10000         2494.414          20.80000\r\n6        0      1       90.10000         2494.414          11.49123\r\n  avg_fin_balance mortgage_age cred_limit\r\n1        1767.197     182.0000      12500\r\n2        1767.197     138.9601          0\r\n3           0.000     138.9601          0\r\n4        1021.000     138.9601          0\r\n5         797.000      93.0000          0\r\n6        4953.000     138.9601          0\r\n```\r\n\r\nNotes: Before we start exercises, lets get familiar with a new data set. The \"csale\" data set consists of anonymized information from the insurance industry.  The outcome, \"purchase\", represents whether customers purchased insurance following a direct mail campaign. The other variables consist of information from credit reports of those customers.  This is a small subset of all the variables and data in this set, which is available in the \"Information\" package. We'll be using this data to model predictors of purchasing behavior.\r\n\r\n---\r\n\r\n# Let's practice!\r\n\r\nNotes: Now let's fit and interpret some GAMs.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",fields:{slug:"/chapter4_01"}}},{node:{rawMarkdownBody:'\r\n# Making predictions\r\n\r\nNoam Ross \r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: GAMs are great for understanding complex systems, and also for making predictions. Here we\'ll learn to make predictions from the models we fit, and to explain how the model creates them.\r\n\r\n---\r\n\r\n# mgcv\'s `predict()` function\r\n\r\n```r\r\npredict(log_mod2)\r\n```\r\n\r\n```out\r\n            1             2             3             4 \r\n-0.8672827973 -2.9135420237 -0.4839780158 -0.1996086132 \r\n            5             6             7             8 \r\n-0.4416783066 -1.2351679544 -0.6148559122 -2.9135420237 \r\n...\r\n```\r\n\r\nNotes: As with most models in R, you can make predictions from a GAM object with the predict() function.  Simply running predict() on a model, in this case our logistic model of purchasing behavior, will yield a vector of predictions for each data point in the data set we used to fit the model.\r\n\r\n---\r\n\r\n# Prediction types\r\n\r\n```r\r\npredict(log_mod2, type = "link")\r\n```\r\n\r\n```out\r\n            1             2             3             4 \r\n-0.8672827973 -2.9135420237 -0.4839780158 -0.1996086132 \r\n            5             6             7             8 \r\n-0.4416783066 -1.2351679544 -0.6148559122 -2.9135420237 \r\n...\r\n```\r\n\r\n```r\r\npredict(log_mod2, type="response")\r\n```\r\n\r\n```out\r\n         1          2          3          4 \r\n0.29582001 0.05148818 0.38131322 0.45026288 \r\n         5          6          7          8 \r\n0.39134114 0.22527819 0.35095230 0.05148818 \r\n...\r\n```\r\n\r\n```r\r\nplogis(predict(log_mod2, type="link"))\r\n```\r\n\r\nNotes: By default, the predict() function returns values on the "link" scale.  That is, the scale on which the model was fit to the data. For a logistic model, this is the log-odds scale.  We can have predict() return results on the probability scale by using the argument type = "response".  This is the equivalent of running the plogis() logistic function on our predictions.\r\n\r\n---\r\n\r\n# Standard errors\r\n\r\n```r\r\npredict(log_mod2, type = "link", se.fit = TRUE)\r\n```\r\n\r\n```out\r\n$fit\r\n         1          2          3          4 \r\n-0.8672828 -2.9135420 -0.4839780 -0.1996086 \r\n         5          6          7          8 \r\n-0.4416783 -1.2351680 -0.6148559 -2.9135420 \r\n\r\n$se.fit\r\n        1         2         3         4 \r\n0.2850848 0.1646090 0.2299404 0.2159088 \r\n        5         6         7         8 \r\n0.2767443 0.7601131 0.2454877 0.1646090 \r\n```\r\n\r\n\r\nNotes: If we set the argument se.fit to TRUE in our call, predict() returns a list where the first element, fit, contains our vector of predictions, and the second element, named se.fit, contains standard errors for our predictions.\r\n\r\n---\r\n\r\n# Standard errors (2)\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/logerrs-1.png?raw=true)\r\n\r\n\r\nNotes: Standard errors are only approximations when we use the probability scale. This is because errors are non-symmetrical on this scale.  If you use standard errors to construct confidence intervals for your predictions, you should do so on the log-odds scale, and then convert them to probability using the plogis() logistic function.\r\n\r\nHere, on the left, we see what happens when we make intervals by adding or subtracting on the response scale. Predictions of probability can be below zero or above one, which doesn\'t make any sense. On the right, we show the result of doing this correctly.  When we transform from log-odds to probability after adding the errors, our predictions are well bounded.\r\n\r\n---\r\n\r\n# Predictions on new data\r\n\r\n```r\r\ntrained_model <- gam(response ~ s(predictor),\r\n                     data = train_df,\r\n                     family = binomial,\r\n                     method = "REML")\r\n```\r\n\r\n\r\n```r\r\n# Test data\r\ntest_predictions <- predict(trained_model,\r\n                            type = "response",\r\n                            newdata = test_df) \r\n```\r\n\r\nNotes: Of course, we are often interested on model predictions beyond the data we use to fit the model, that is the new, out-of sample data.  The newdata argument lets us pass new data to our model and generate predictions, so we can apply our model to new situations. \r\n\r\nThis allows us to predict on test data after fitting our model on training data.\r\n\r\n---\r\n\r\n# Explaining predictions by terms\r\n\r\n```r\r\npredict(log_mod2, type = "terms")\r\n```\r\n\r\n```out\r\n      s(n_acts) s(bal_crdt_ratio) s(avg_prem_balance) s(retail_crdt_ratio) s(avg_fin_balance) s(mortgage_age) s(cred_limit)\r\n1     1.2115213      0.3327855673        -0.135920526         0.0678994892       -0.040572487   -0.2918390343    -0.3705562\r\n2    -0.8850186     -0.4058818961        -0.135920526        -0.0075325272       -0.040572487   -0.0209184283     0.2229033\r\n3     0.5693622      0.2972364048        -0.135920526         0.0678994892        0.156060412   -0.0209184283     0.2229033\r\n4     0.8974704      0.3827671103        -0.135920526         0.0626482277        0.032042157   -0.0209184283     0.2229033\r\n5     0.8974704     -0.0727464938        -0.135920526        -0.0686510698        0.065216680    0.2906502573     0.2229033\r\n6    -0.6228781      0.1936974771        -0.135920526        -0.0075325272        0.776081676   -0.0209184283     0.2229033\r\n7     0.3642246      0.3377181800        -0.135920526        -0.0075325272       -0.040572487    0.2849244308     0.2229033\r\n8    -0.8850186     -0.4058818961        -0.135920526        -0.0075325272       -0.040572487   -0.0209184283     0.2229033\r\n9     1.0209905      0.3604064595         0.317309246        -0.0253158695       -0.040572487    0.3551533139    -0.3230903\r\n10    1.7675666     -0.4533384774         0.346837355         0.0377046376        0.150927175    0.1269458733    -0.5366686\r\n```\r\n\r\nNotes: In multiple regression, it is often useful to understand how each term contributes to an individual prediction. We can examine this by setting the type argument to "terms" in the predict() function.  This will produce a matrix showing the contribution of each smooth to each prediction.\r\n\r\nIf we were to sum across all the columns of this matrix, and add the intercept, we would have our overall prediction the log-odds scale.\r\n\r\n---\r\n\r\n# Explaining predictions by terms (2)\r\n\r\n```r\r\npredict(log_mod2, type = "terms")[1, ]\r\n```\r\n\r\n```out\r\n           s(n_acts)    s(bal_crdt_ratio) \r\n          1.21152126           0.33278557 \r\n s(avg_prem_balance) s(retail_crdt_ratio) \r\n         -0.13592053           0.06789949 \r\n  s(avg_fin_balance)      s(mortgage_age) \r\n         -0.04057249          -0.29183903 \r\n       s(cred_limit) \r\n         -0.37055621 \r\n```\r\n\r\n```r\r\nplogis(\r\n  sum(predict(log_mod2, type = "terms")[1, ]) + coef(log_mod2)[1]\r\n)\r\n```\r\n```out\r\n[1] 0.29582 \r\n```\r\n\r\n\r\nNotes: Here we look at the first row of this output to see the role of each term in influencing our prediction probability.  This allows us to explain model predictions.  \r\n\r\nFor instance, as we can see for this one data point, the number of accounts has about four times the effect in increasing purchase probability prediction than balance-credit ratio.  Mortgage age and credit limit influence the prediction in the opposite direction, about the same amount as balance-credit ratio.\r\n\r\nIf we add these terms up, add the intercept, and transform using the plogis() function, we get this data point\'s predicted purchase probability.\r\n---\r\n\r\n# Let\'s practice!\r\n\r\nNotes: Now let\'s make and interpret some predictions.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',fields:{slug:"/chapter4_09"}}},{node:{rawMarkdownBody:'\r\n# Basis Functions and Smoothing\r\n\r\nNoam Ross\r\nSenior Research Scientist, EcoHealth Alliance\r\n\r\nNotes: Now that you have a sense of how GAMs can fit nonlinear data, let\'s learn a bit more about how they work. GAMs are powerful because of their ability to take on many shapes, but this is also what makes them challenging. Their flexibility makes it easy to over-fit your da\r\n\r\n---\r\n\r\n# Getting the right fit\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/thin-gam-fit-1.png?raw=true)\r\n\r\n-  Close to the data (avoiding under-fitting)\r\n-  Not fitting the noise (avoiding over-fitting)\r\n\r\nNotes: In general, we want to balance two things when fitting a nonlinear model.  We want a model that captures the relationship by being close to the data, but we also want to avoid fitting our model to noise, or over-fitting.\r\n\r\n---\r\n\r\n# Balancing Wiggliness\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula1-chapter1.png?raw=true)\r\n\r\nNotes: How well the GAM captures patterns in the data is measured by a term called likelihood.  Its complexity, or how much the curve changes shape, is measured by wiggliness. The key to a good fit is the trade-off between the two. This trade-off is expressed by this simple equation, with a smoothing parameter, or lambda value, controlling the balance. This smoothing parameter is optimized when R fits a GAM to data.\r\n\r\n---\r\n\r\n# Choosing the Right Smoothing Parameter\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/diffsmooth-1.png?raw=true)\r\n\r\nNotes: Here are plots of three GAMs with different smoothing, or lambda values.  As you can see, the one on the left smooths too much, creating a straight line through curved data.  The one in the middle smooths too little, fitting noise rather than the trend.  The one on the right is just right. It\'s lambda value balances over-and-under-fitting.\r\n\r\n---\r\n\r\n# Smoothing Syntax\r\n\r\nSetting a fixed smoothing parameter\r\n\r\n```r\r\ngam(y ~ s(x), data = dat, sp = 0.1)\r\ngam(y ~ s(x, sp = 0.1), data = dat)\r\n```\r\nSmoothing via restricted maximum likelihood\r\n\r\n```r\r\ngam(y ~ s(x), data = dat, method = "REML")\r\n```\r\n\r\nNotes: Normally when we fit a model with mgcv\'s gam() function, we let the package do the work of selecting a smoothing parameter.  However, we can fix the smoothing parameter to a value of our choosing via the sp argument.  The sp argument can be set for the whole model via an argument to the gam() function, as in the first line of code.  We can also set the sp argument for a specific term in the GAM formula, as shown in the second line of code.\r\n\r\nInstead if we allow R to do this work for us, the mgcv package offers several different methods for selecting smoothing parameters.  I, and most GAM experts, strongly recommend that you fit models with the REML, or "Restricted Maximum Likelihood" method.  While different methods have their advantages, REML is most likely to give you reliable, stable results. \r\n\r\n---\r\n\r\n# Number of basis functions\r\n\r\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/diffbasis-1.png?raw=true)\r\n\r\nNotes: In addition to the smoothing parameter, the other factor that affects how wiggly a GAM function can be is the number of basis functions that make up a smooth function.  Here I\'ve plotted GAMs with 3, 7, and 12 basis functions all fit to the same data.\r\n\r\nAs you can see, a smooth with a small number of basis functions is limited in its wiggliness, while one with many basis functions is capable of capturing finer patterns.\r\n\r\n---\r\n\r\n# Basis Function Syntax\r\n\r\nSetting number of basis functions\r\n\r\n```r\r\ngam(y ~ s(x, k = 3), data = dat, method = "REML")\r\n    \r\ngam(y ~ s(x, k = 10), data = dat, method = "REML")\r\n```\r\n\r\nUse the defaults\r\n\r\n```r\r\ngam(y ~ s(x), data = dat, method = "REML")\r\n```\r\n\r\nNotes: To set the number of basis functions in a smooth, we use the k argument in the smooth function in a GAM formula.  Setting this value too low will prevent the model from being sufficiently wiggly.  If it\'s high, though, the automatic smoothing parameter selection will prevent it from being too wiggly. We just don\'t want to set it very high, which can result in a model with more parameters than data, or one that is slow to fit.\r\n\r\nLater, in the model testing portion of the course, we will learn how to test if the number of basis functions are adequate to fit our data.\r\n\r\n---\r\n\r\n# Let\'s practice!\r\n\r\nNotes: Now, let\'s explore GAMs with different smoothing parameters and different numbers of basis functions.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n',fields:{slug:"/chapter1_05"}}},{node:{rawMarkdownBody:"\n# Checking concurvity\n\nNoam Ross\nSenior Research Scientist, EcoHealth Alliance\n\nNotes: Now we'll learn about another area that's important to check in GAMs: concurvity.\n\n---\n\n# Collinearity\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/pairs-1.png?raw=true) \x3c!-- .element: style=\"max-height:525px;\" --\x3e\n\n\nNotes: You may recall the concept of collinearity from a linear modeling course.  When two variables or covariates in a model are strongly correlated, it's difficult to fit the model, because the outcome variable could be responding to either one.  We call this phenomenon collinearity, and it can result in poorly fit models with large confidence intervals.  In general, we avoid putting multiple collinear variables into the same model.\n\nHere, we look at correlation between three covariates in our mpg data set: the length, width, and height of cars.  We can see that there is strong collinearity between car length and width. It would be hard to distinguish between their effects on car efficiency.\n\n---\n\n# Concurvity\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/concurv-demo-1.png?raw=true)\n\nNotes: With GAMs, we have an additional potential pitfall. Even if two variables aren't collinear, they may have concurvity, that is, one may be a smooth curve of another.  For instance, on the left, we have two covariates, X1 and X2, that are not linearly related but form a perfect parabola.  If we use both X1 and X2 as predictors in a model, we get smooths with wild confidence intervals, as shown in the middle and right plots.\n\n---\n\n# The `concurvity()` function\n\n<div  class='left' style='float:left;width:35%'>\n<p data-markdown>![](https://github.com/noamross/gams-in-r-course/blob/master/images/concurv-demo2-1.png?raw=true)</p>\n</div>\n<div class='right' style='float:right;width:65%'>\n<p data-markdown>\n```r\nconcurvity(m1, full = TRUE)\n```\n</p>\n<p data-markdown>\n```out\n            para s(X1) s(X2)\nworst       0    0.84  0.84\nobserved    0    0.22  0.57\nestimate    0    0.28  0.60\n```\n</p>\n</div>\n\n\n\nNotes: mgcv's concurvity() function measures concurvity in model variables.  Like gam.check(), we run this function on a model object to examine the quality of our model.  \n\nconcurvity() has two modes.\n\nThe first mode, full = TRUE, reports overall concurvity for each smooth. Specifically, it shows how much each smooth is predetermined by all the other smooths. \n\nSince concurvity is complex, the function reports three different ways of measuring concurvity.  Each is better in some situations.  What is important is that you should always look at the worst case, and if the value is high (say, over 0.8), inspect your model more carefully.\n\nHere I show the output of running the concurvity() function on a model with variables that are related but not perfectly. The concurvity of the terms is high in the worst case, so we'll want to inspect the plots of our model closely and be careful in making interpretations.\n\n---\n\n# Pairwise concurvities\n\n```r\nconcurvity(model, full = FALSE)\n```\n\n```out\n$worst\n      para s(X1) s(X2)\npara     1  0.00  0.00\ns(X1)    0  1.00  0.84\ns(X2)    0  0.84  1.00\n\n$observed                |  $estimate\n      para s(X1) s(X2)   |        para s(X1) s(X2)\npara     1  0.00  0.00   |  para     1  0.00   0.0\ns(X1)    0  1.00  0.57   |  s(X1)    0  1.00   0.6\ns(X2)    0  0.22  1.00   |  s(X2)    0  0.28   1.0\n\n\n```\n\nNotes: If any of these values from the full = TRUE mode is high, we will want to also use the second mode, setting full = FALSE.  With full = FALSE, the function returns matrices of\npairwise concurvities.  These show the degree to which each variable is predetermined by each other variable, rather than all the other variables.  This can be used to pinpoint which variables have a close relationship.  Once again, the function returns three measures, this time as three matrices.  Look for the worst-case scenario and see if variables with high values have problematic shapes or confidence intervals.\n\n---\n\n# Let's practice!\n\nNotes: Now let's try some examples.\n\n\n",fields:{slug:"/chapter2_11"}}},{node:{rawMarkdownBody:"\n# 2-Dimensional Smooths and Spatial Data\n\nNoam Ross \nSenior Research Scientist, EcoHealth Alliance\n\nNotes: Up until now, we have been working with models made up of one or several smooths, each of a single variable.  Now, we will expand our models to include smooths of multiple variables and their interactions.  This will allow us to look at new kinds of data, especially geospatial data, which are best represented by complex surfaces rather than single smooth lines.\n\n---\n\n# Interactions\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula1-chapter3.png?raw=true)\n\nNotes: You may be familiar with the concept of interactions from linear modeling.  Interactions in models represent the fact that outcomes depend on non-independent relationships of multiple variables.  In a linear model, they are generally represented by adding a term multiplying two variables.  This can result in the outcome being higher or lower than what would be predicted by the sum of the two values alone.\n\n---\n\n# Interactions in GAMs\n\n<div  class='left' style='float:left;width:25%'>\n<p data-markdown>\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula2-chapter3.png?raw=true)\n</p>\n</div>\n<div class='right' style='float:right;width:75%'>\n<p data-markdown>\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/interactions-1-1.png?raw=true)\n</p>\n</div>\n\n\n\nNotes: In a GAM, the relationship between a variable and an outcome changes across the range of the smooth. Similarly, interactions are different across all the values of two or more variables. We represent interactions between variables as a smooth surface, so any combination of variables can take a different value. This is also a natural way to represent spatial data.\n\n---\n\n# Syntax for interactions\n\n```r\ngam(y ~ s(x1, x2), # <-- 2 variables \n        data = dat, method = \"REML\")\n```\n\nNotes: The syntax for interactions in GAMs is straightforward.  To model the interaction between two variables, we put two variables inside the s() function in a GAM formula, as shown here.\n\n\n---\n\n# Mixing interaction and single terms\n\n```r\ngam(y ~ s(x1, x2) + s(x3),\n        data = dat, method = \"REML\")\n``` \n\n```r\ngam(y ~ s(x1, x2) + x3 + x4,\n        data = dat, method = \"REML\")\n```\n\nNotes: You can mix interactions with other terms, which can be linear or nonlinear.  For instance, the first formula here has an additional nonlinear term, x3, which is separate from the interaction of terms x1 and x2.  The second formula has linear terms x3 and x4.  Just as in our previous GAMs, you can include discrete, categorical terms along with interactions and linear terms.\n\nA common way to model geospatial data is to use an interaction term of x and y coordinates, along with individual terms for other predictors. The interaction term then accounts for the spatial structure of the data.\n\n---\n\n# Interaction model outputs\n\n```out\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.34256    0.01646   20.82   <2e-16 ***\n ---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(x1,x2) 10.82   14.9 14.37  <2e-16 *** #<-- Interaction\n ---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.519   Deviance explained = 54.5%\nGCV = 0.057564  Scale est. = 0.054161  n = 200\n```\n\nNotes: When you look at the summary outputs of a model with interactions, you'll see that the interaction is a single smooth term.  This combines the effects of x1, x2, and their\ncombination in a single smooth.  This differs from what you may expect in a linear model, where terms for x1, x2, and their combination are separate. We will discuss how to fit a model that separates these components later in this chapter.\n\nAlso note the high EDF, that is, effective degrees of freedom for this term. It takes many more basis functions, and therefore more data, to build a two-dimensional surface rather than a one-dimensional line.\n\n---\n\n# Spatial data\n\n```r\nmeuse\n```\n\n```out\n         x      y cadmium copper lead zinc   elev       dist   om ffreq soil lime landuse dist.m\n1   181072 333611    11.7     85  299 1022  7.909 0.00135803 13.6     1    1    1      Ah     50\n2   181025 333558     8.6     81  277 1141  6.983 0.01222430 14.0     1    1    1      Ah     30\n3   181165 333537     6.5     68  199  640  7.800 0.10302900 13.0     1    1    1      Ah    150\n4   181298 333484     2.6     81  116  257  7.655 0.19009400  8.0     1    2    0      Ga    270\n5   181307 333330     2.8     48  117  269  7.480 0.27709000  8.7     1    2    0      Ah    380\n6   181390 333260     3.0     61  137  281  7.791 0.36406700  7.8     1    2    0      Ga    470\n7   181165 333370     3.2     31  132  346  8.217 0.19009400  9.2     1    2    0      Ah    240\n8   181027 333363     2.8     29  150  406  8.490 0.09215160  9.5     1    1    0      Ab    120\n9   181060 333231     2.4     37  133  347  8.668 0.18461400 10.6     1    1    0      Ab    240\n10  181232 333168     1.6     24   80  183  9.049 0.30970200  6.3     1    2    0       W    420  \n```\n\n```r\n?sp::meuse\n```\n\nNotes: For exercises involving interactions in GAMs, we will use a new data set called \"meuse\".  This is geospatial data of heavy metal soil pollution along the Meuse river in the Netherlands. It consists of a data frame with x and y coordinates, measures of heavy metals in the soil, and other spatial covariates such as elevation, distance from the river, and the land-use type occurring in that location. For more information on the source and variables of these data, you can look at the help file for this data set in the sp package.\n---\n\n# Let's practice!\n\nNotes: Now let's try some examples of two-dimensional modeling with GAMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",fields:{slug:"/chapter3_01"}}},{node:{rawMarkdownBody:"\n# Interactions with Different Scales\n\nNoam Ross\nSenior Research Scientist, EcoHealth Alliance\n\nNotes: We have learned about two kinds of two-way interactions: two-dimensional interaction smooths, and categorical-continuous interactions.  Now we will learn about another kind: tensor smooths. Tensor smooths let us model interactions that operate on different scales, such as space and time.\n\n---\n\n# Interactions with one smoothing parameter\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula3-chapter3.png?raw=true)\n\nNotes: Earlier in this chapter, we discussed 2D smooths of the form s(x1, x2).  As with all GAM smooths, this one has a smoothing parameter, or lambda value, that defines wiggliness. There is a single lambda value for the whole 2D smooth.\n\n---\n\n# Variables with different scales or wiggliness\n\n\nNumeric terms from `meuse` on different scales:\n\n```out\n        x      y  elev    om\n 1 181072 333611  7.91   13.6\n 2 181025 333558  6.98   14  \n 3 181165 333537  7.8    13  \n 4 181298 333484  7.66   8  \n 5 181307 333330  7.48   8.7\n 6 181390 333260  7.79   7.8\n 7 181165 333370  8.22   9.2\n 8 181027 333363  8.49   9.5\n 9 181060 333231  8.67   10.6\n10 181232 333168  9.05   6.3\n```\n\nNotes: However, there are lots of situations where having a single smoothing parameter does not make sense.  Take our meuse data set.  It has variables for x and y, as well as elevation.  All are in meters.  We would expect the scale of change to be similar horizontally along x and y, but it could be much different on a per-meter basis with elevation, where small differences could mean very different environments.  The terms would likely not have the same wiggliness.  Similarly, it would not make sense to use the same wiggliness to model distance and the om variable, which is organic matter measured in grams per kilogram.  They have incomparable units of measure.\n\n---\n\n# Tensor Smooths\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula4-chapter3.png?raw=true)\n\n```r\ngam(y ~ te(x1, x2), data = data, method = \"REML\")\n```\n\n```r\ngam(y ~ te(x1, x2, k = c(10, 20)), data = data, method = \"REML\")\n```\n\n\nNotes: Tensor smooths are more appropriate for interactions of variables with different scales or units.  A tensor is similar to a regular two-dimensional smooth, but it has two smoothing parameters, one for each variable.\n\nTensor smooths are used similarly to regular smooths, one just uses te() instead of s() for them.  Since there are multiple smoothing parameters, you can specify a different number of basis functions, or k values, for each smooth.\n\n---\n\n# Example: Fitting with Tensors\n\n![compte-1.png](https://github.com/noamross/gams-in-r-course/blob/master/images/compte-1.png?raw=true)\n\n\nNotes: Here's an example that shows how tensors can be a much better tool in some cases.  On the left we're showing the actual relationship between two variables, x1 and x2, and an outcome.  x1 and x2 are on very different scales - x1 varies on a range of less than 5% of x2.  A model fit using s(), which assumes both variables vary similarly, comes out looking like the plot in the middle.  However, if we use tensor smooths, allowing the variables to have different smoothing parameters, we get the model on the right, a much better fit.\n\n---\n\n# Tensor interactions\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/formula5-chapter3.png?raw=true)\n\n```r\ngam(y ~ s(x1) + s(x2) + ti(x1, x2), data = data, method = \"REML\")\n\n```\n\nNotes: One other great advantage of tensor smooths is that they can be used to separate out interactions from individual univariate effects.  Using tensor interactions, we can model only the interaction of two variables, and not their independent effects, which we estimate separately.\n\nTo fit a model this way, we use regular smooth terms, or s() functions, to model each univariate smooth, and then tensor interactions, or ti() functions, for each interaction.\n\nNote that each of these components has its own smoothing parameters and can have its own number of basis functions.  This means we are estimating more parameters. Necessarily, such models need more data.\n\n---\n\n# Example: Tensor Interactions\n\n```out\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1) + s(x2) + ti(x1, x2)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.318698   0.008697   36.65   <2e-16 ***\n ---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n            edf Ref.df     F  p-value    \nte(x1)      4.93  6.009 23.16  < 2e-16 ***    # Separate terms for \nte(x2)      3.42  4.242 10.35 2.75e-08 ***    # each variable and\nti(x1,x2) 10.15 12.763 16.08  < 2e-16 ***     # the interaction\n ---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.444   Deviance explained = 46.5%\n-REML = -85.566  Scale est. = 0.037067  n = 500\n```\n\nNotes: Here's the summary output of fitting a model with tensor interactions.  As you can see, we now have separate smooth terms in the model for each variable as well as the interaction.  This allows us to evaluate the significance of these components independently.\n\nThis is another advantage of tensor smooths.  Even if two terms are not on different scales,\nusing tensors allows us to separate their individual effects from their interaction.\n\n---\n\n# Example: Tensor Interactions\n\n```r\ngam(y ~ s(x1) + s(x2) + ti(x1, x2), data = data, method = \"REML\")\n```\n\n![tei-1.png](https://github.com/noamross/gams-in-r-course/blob/master/images/tei-1.png?raw=true)\n\nNotes: Here are plots of this model.  We have univariate smooths for x1 and x2, and a surface representing the interaction.  The univariate smooths are additive, and then the interaction is an addition effect on top of those.  Separating the effects in this way makes complex models more understandable.\n\n---\n\n# Let's practice!\n\nNotes: Let's use what we've learned about tensor smooths to make improved models of the meuse data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",fields:{slug:"/chapter3_11"}}},{node:{rawMarkdownBody:"\n# Visualizing Logistic GAMs\n\nNoam Ross\nSenior Research Scientist, EcoHealth Alliance\n\nNotes: Logistic GAMs are very useful for classification.  However, to make the most of these models, we need to visualize logistic GAM outputs in an interpretable way.  Here we'll learn how to modify plotting commands so as to make our logistic visualizations easy to understand.\n---\n\n# Transforming Scales\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/logistic-1.png?raw=true)\n\nNotes: In the last lesson we learned that when building GAMs for binary classification, outputs are in log-odds and need to be transformed by the logistic function to be interpreted as probabilities.\n\n---\n\n# Log-odds plots\n\n```r\nplot(binom_mod)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/untrans-1.png?raw=true)\n\nNotes: When we plot the output of a logistic GAM, we see the partial effect of smooths on the log-odds scale.  It can be difficult to interpret this.  We understand the shape of the effect, but the magnitude of the effect on probability is not immediately apparent.\n\n---\n\n# Converting partial effects\n\n```r\nplot(binom_mod, pages = 1, trans = plogis)\n```\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/trans-1.png?raw=true)\n\nNotes: When plotting, we can convert our output to the probability scale by using the trans argument. The trans argument takes a function that transforms the output.  So we can pass the plogis() logistic function to this argument, and all values in plots will be transformed from log-odds to probabilities.\n\nAs you can see, the outputs are now on a probability scale between zero and one.  You can see the first term, which was previously linear, has a slight curve on this scale.\n\n---\n\n# Converting partial effects (2)\n\n```r\nplot(binom_mod, pages = 1, trans = plogis)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/translin-1.png?raw=true)\n\nNotes: When we transform this way, we see our partial effects are all centered on an average value of 0.5.  This is because we are looking at each partial effect with no intercept.\n\n---\n\n# Adding an intercept\n\n```r\nplot(binom_mod, pages = 1, trans = plogis,\n     shift = coef(binom_mod)[1])\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/wintercept-1.png?raw=true)\n\n\nNotes: To incorporate the model intercept, we can use the shift argument that we learned earlier. shift adds its value to all model outputs, before the transformation in the function we pass to trans.  So we shift our outputs by passing the intercept,  extracting it from the model object with the coef() function.\n\n---\n\n# Adding an intercept (2)\n\n```r\nplot(binom_mod, pages = 1, trans = plogis,\n     shift = coef(binom_mod)[1])\n```\n\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/winterceptlin-1.png?raw=true)\n\nNotes: Now our partial effect plots have much more intuitive interpretation.  Each partial effect plot can be interpreted as showing the probability of the outcome if all other variables were at their average value.  At their own average value, you get only the effect of the intercept.\n\nHere, we can see that the variable x1 affects the probability of the outcome, lowering it to just over 0.5 when x1's value is zero, to just over 0.8 when it's value is 1. x2's nonlinear effect is greater, pushing the outcome probability down to near 0.2 at its highest value.\n\n---\n\n# Incorporating intercept uncertainty\n\n```r\nplot(binom_mod, pages = 1, trans = plogis,\n     shift = coef(binom_mod)[1],\n     seWithMean = TRUE)\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/wintercepterr-1.png?raw=true)\n\nNotes: Earlier we learned about the seWithMean argument, which adds the intercept uncertainty to the smooth uncertainty. It is natural to include this uncertainty here, as we are adding the intercept term.  \n\nNow, the confidence intervals in our partial effect plots also have a natural interpretation.  They  may be interpreted as the range of uncertainty of the probability of the outcome for any value of the variable, holding other variables equal at their average value.\n\n---\n\n# Improving the plot\n\n```r\nplot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1],\n     seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = \"lightgreen\", \n     col = \"purple\")\n```\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/winterceptcol-1.png?raw=true)\n\nNotes: Finally, we can always improve our plot by changing visual options as we did with earlier plots.\n\n---\n\n# Let's practice!\n\nNotes: Now let's make some plots of logistic models and interpret what they mean.\n\n\n\n\n\n\n\n\n\n\n",fields:{slug:"/chapter4_04"}}},{node:{rawMarkdownBody:"\n# Doing more with GAMs\n\nNoam Ross\nSenior Research Scientist, EcoHealth Alliance\n\nNotes: Congratulations!  You've have successfully completed this course on Generalized Additive Models.  Let's quickly review what you've learned, and point you towards a few resources for you to use as you apply these models in your future work.\n\n---\n\n# Course review\n\n**Chapter 1**\n\n- GAM theory\n- Fitting GAMs\n- Mixing linear and nonlinear terms\n\n**Chapter 2**\n\n- Interpreting GAMs\n- Visualizing GAMs\n- Model-checking and concurvity\n\nNotes: In the first chapter of the course, you learned the basic theory of how smooths are constructed and how to assemble GAMs from multiple smooths and linear terms.  In chapter two, you learned how to interpret GAM model outputs, plot partial effects to check your models for problems and how to fix them. \n\n---\n\n# Course review\n\n**Chapter 3**\n\n- 2-D Interactions and spatial data\n- Interactions with different scales\n- Continuous-categorical interaction\n\n**Chapter 4**\n\n- Logistic GAMs\n- Plotting logistic outputs\n- Making predictions\n\nNotes: Then, in chapter three, you expanded to building and visualizing GAMs with interactions of multiple variables, including geospatial GAMs and GAMs with continuous-categorical interactions.  Finally, in chapter 4, you learned how to use logistic GAMs for binary classification and prediction.  Altogether, you've created a toolbox for using GAMs to model many different types of data and problems.\n\nNow I'll point you to a few more resources for using GAMs in your work.\n\n---\n\n# GAMs and the Tidyverse\n\n```r\nlibrary(broom)\n\naugment(gam_model)\ntidy(gam_model)\nglance(gam_model)\n```\n\n```r\nlibrary(caret)\n\ntrain(x, y, method = \"gam\", ...)\n```\n\nNotes: In this course we did not make use of tools from the popular set of R packages known as the tidyverse, but GAMs work readily with tidy tools.\n\nIf you are familiar with the broom package, you'll know of general functions like augment(), tidy(), and glance().  All these functions work well with GAMs, giving you tidy ways of inspecting, evaluating, and predicting from your models.\n\nSimilarly, if you use the caret package for predictive modeling, you can pass \"gam\" to the method argument and caret will fit models with the mgcv package.\n\n---\n\n# Other types of smooths\n\n```r\n?smooth.terms\n```\n\nNotes: As you continue to use GAMs, you'll discover mgcv has many additional capabilities.  The package also has many extended help files on specific topics that will be useful to explore.\n\nFirst, there are many additional types of smooths beyond the ones we have used here.  These can be useful in specific situations, such as geospatial modeling or for seasonal time series.  You'll find these described in the ?smooth.terms help file.\n\n---\n\n# Other types of outcomes/distributions\n\n```r\n?family.mgcv\n```\n\nNotes: If you have taken a course in generalized linear models, you probably know that there are many types of outcomes, such as count data, which they can model. These can be fit with GAMs, as well. mgcv also has an extensive collection of outcome distributions above and beyond those available in most GLM packages. You'll find them described in the ?family.mgcv help file.  If you have not taken it already, you may want to consider DataCamp's Generalized Linear Models course.\n\n---\n# Variable selection\n\n```r\n?gam.selection\n```\n\nNotes: mgcv has tools for variable selection when model building.  You can learn about these in the ?gam.selection help file.\n\n---\n\n# Complex model structures\n\n```r\n?gam.models\n``` \n\nNotes: Finally, there are other options in mgcv for alternative or more complex model structures, such as mixed effects. The ?gam.models help file has an overview of these topics and will point you to more documentation.  If you are interested in these more complex models, the course on Hierarchical and Mixed Effects Models here on DataCamp may be of interest to you.\n\n---\n\n# More Resources\n\nGo to <http://bit.ly/gam-resources> for a collection of:\n\n- Videos\n- Books\n- Papers\n- Other courses\n\nabout GAMs!\n\nNotes: I've collected some of the my resources for learning about GAMs so you can continue learning. If you want to extend your knowledge, practice more, have a handy reference, or learn about GAMs in a different way, go to <http://bit.ly/gam-resources> links to a variety of GAM-related material by expert statisticians instructors. \n\n---\n\n#  Thank You!\n\nNotes: Thanks for taking this course on nonlinear modeling in R with GAMs. I hope you find these flexible and powerful tools useful in your work.\n\n",fields:{slug:"/chapter4_12"}}}]}}}}}]);
//# sourceMappingURL=component---src-templates-chapter-js-864b805e2c56d56463ac.js.map